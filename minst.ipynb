{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mnist\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\__internal__\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\__internal__\\backend\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _initialize_variables \u001b[38;5;28;01mas\u001b[39;00m initialize_variables\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m track_variable\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\__init__.py:21\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\applications\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras Applications are premade architectures with pre-trained weights.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtBase\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtLarge\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtSmall\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\applications\\convnext.py:26\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"ConvNeXt models for Keras.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03mReferences:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m  (CVPR 2022)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initializers\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\__init__.py:54\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\__init__.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\v1\\__init__.py:49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lite\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lookup\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\lite\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpsSet \u001b[38;5;66;03m# line: 169\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m toco_convert \u001b[38;5;66;03m# line: 1039\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\lite\\experimental\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.lite.experimental namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m authoring\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalyzer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelAnalyzer \u001b[38;5;28;01mas\u001b[39;00m Analyzer \u001b[38;5;66;03m# line: 35\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpreter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpResolverType \u001b[38;5;66;03m# line: 303\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\lite\\experimental\\authoring\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.lite.experimental.authoring namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauthoring\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauthoring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compatible \u001b[38;5;66;03m# line: 265\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_wrapper \u001b[38;5;28;01mas\u001b[39;00m _module_wrapper\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m], _module_wrapper\u001b[38;5;241m.\u001b[39mTFModuleWrapper):\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\lite\\python\\authoring\\authoring.py:44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lite\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m converter_error_data_pb2\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export \u001b[38;5;28;01mas\u001b[39;00m _tf_export\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecodeError\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph_pb2 \u001b[38;5;28;01mas\u001b[39;00m _graph_pb2\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmicrofrontend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_microfrontend_op  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m conversion_metadata_schema_py_generated \u001b[38;5;28;01mas\u001b[39;00m conversion_metdata_fb\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lite_constants \u001b[38;5;28;01mas\u001b[39;00m constants\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\lite\\experimental\\microfrontend\\python\\ops\\audio_microfrontend_op.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resource_loader\n\u001b[1;32m---> 24\u001b[0m _audio_microfrontend_op \u001b[38;5;241m=\u001b[39m \u001b[43mload_library\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_op_library\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresource_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path_to_datafile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_audio_microfrontend_op.so\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maudio_microfrontend\u001b[39m(audio,\n\u001b[0;32m     29\u001b[0m                         sample_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m,\n\u001b[0;32m     30\u001b[0m                         window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m                         out_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     50\u001b[0m                         out_type\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39muint16):\n\u001b[0;32m     51\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Audio Microfrontend Op.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m  This Op converts a sequence of audio data into one or more\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m    ValueError: If the audio tensor is not explicitly a vector.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\load_library.py:57\u001b[0m, in \u001b[0;36mload_op_library\u001b[1;34m(library_filename)\u001b[0m\n\u001b[0;32m     54\u001b[0m lib_handle \u001b[38;5;241m=\u001b[39m py_tf\u001b[38;5;241m.\u001b[39mTF_LoadLibrary(library_filename)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m   wrappers \u001b[38;5;241m=\u001b[39m _pywrap_python_op_gen\u001b[38;5;241m.\u001b[39mGetPythonWrappers(\n\u001b[1;32m---> 57\u001b[0m       \u001b[43mpy_tf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_GetOpList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib_handle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m   \u001b[38;5;66;03m# Delete the library handle to release any memory held in C\u001b[39;00m\n\u001b[0;32m     60\u001b[0m   \u001b[38;5;66;03m# that are no longer needed.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m   py_tf\u001b[38;5;241m.\u001b[39mTF_DeleteLibraryHandle(lib_handle)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan for creating Neural Network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJwAAACbCAYAAACXvfL1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAKbklEQVR4nO3dXUiT7R8H8O/Wv81edKaiNnIpFBREBZJmSVhIVhS9nXRUUSTVDMqgKKxIAqNOLDE6Ka2DMiQqKIgetIxeLBQ6KEuKpITS6MBNLbXc7znwaf/urXTTe9fu6fcDN/jb7tnV9uXadb94XSYRERApYg53A2hsYeBIKQaOlGLgSCkGjpRi4EgpBo6UYuBIKQaOlGLgSKmQBa68vBypqamIiopCZmYmnj9/Hqp/iiKIKRTXUq9du4bNmzfj/PnzyMzMRGlpKaqrq9Hc3IzExMRBX+vxePDp0ydER0fDZDLp3TQKARFBZ2cn7HY7zOYh+jAJgYyMDHE6nd66v79f7Ha7lJSUDPna1tZWAcAtArfW1tYhP1/dv1L7+vrQ2NiI3Nxc72Nmsxm5ubl4+vSp3/69vb1wu93eTXjzSsSKjo4ech/dA/f161f09/cjKSlJ83hSUhLa2tr89i8pKYHNZvNuDodD7yaRIoEMgcJ+lHro0CG4XC7v1traGu4mUQj9T+9fmJCQgHHjxqG9vV3zeHt7O5KTk/32t1qtsFqtejeDDEr3Hs5isSA9PR01NTXexzweD2pqapCVlaX3P0eRZiRHo39TVVUlVqtVKisrpampSfLz8yU2Nlba2tqGfK3L5Qr70Ra34W0ul2vIzzckgRMRKSsrE4fDIRaLRTIyMqS+vj6g1zFwkbsFEriQnPgdCbfbDZvNFu5m0DC4XC7ExMQMuk/Yj1JpbGHgSCkGjpRi4EgpBo6UYuBIKQaOlGLgSCkGjpTS/W4RCp2ioiJNffz4cU3te3t3Tk6Opq6rqwtJu4LBHo6UYuBIKQaOlOIYzsC2bt2qqQ8ePKipPR7PoK832I1AANjDkWIMHCnFwJFSHMMZ2PTp0zV1VFRUmFqiH/ZwpBQDR0oxcKQUx3AG8vsEQACwZ8+eQfd/8+aNpl69erWm9p39wAjYw5FSDBwpxcCRUhzDhVF2dramrqio0NRDzUBw+vRpTf3hwwd9GhZC7OFIKQaOlAo6cA8fPsSaNWtgt9thMplw8+ZNzfMigqNHj2Lq1KmYMGECcnNz8fbtW73aSxEu6DFcd3c35s2bh23btmHDhg1+z586dQpnz57FpUuXkJaWhiNHjiAvLw9NTU2j4lqgnrZs2aKp7Xb7oPs/ePBAU1++fFnvJoVc0IFbuXIlVq5c+cfnRASlpaUoKirC2rVrAQy8KUlJSbh58yY2bdrk95re3l709vZ6a7fbHWyTKILoOoZraWlBW1ub5oy5zWZDZmbmH6fMB/xnMU9JSdGzSWQwugbu17T4gU6ZD3AW87Em7OfhxtIs5gkJCZp627Ztmtr3bxQ6Ojo09YkTJ0LSLpV07eF+TYsf6JT5NPboGri0tDQkJydrpsx3u9149uwZp8wnAMP4Su3q6sK7d++8dUtLC168eIG4uDg4HA7s3bsXJ06cwMyZM72nRex2O9atW6dnuylCBR24hoYGLF261FsXFhYCGDinVFlZiQMHDqC7uxv5+fno6OhAdnY27t69O2bPwaWmpnp/vn79elCvLSsr09T379/Xo0lhFXTgcnJyBv0DW5PJhOLiYhQXF4+oYTQ68VoqKcXAkVJhPw832q1YscL789y5cwfd9/ejewA4c+ZMSNoUTuzhSCkGjpTiV6rOfM83njx58q/7Pnr0SFP73q7kcrl0a5dRsIcjpRg4UoqBI6U4hhuh3y9dAcFdvnr//r2mNuLUDHpjD0dKMXCkFANHSnEMN0LBTmX/u8HO0Y1W7OFIKQaOlGLgSCmO4YI0f/58Tb18+fKAX3vr1i1N3dzcrEeTIgp7OFKKgSOlGDhSimO4IN27d09TT5kyZdD96+vrvT/7Lkc5FrGHI6UYOFKKgSOlOIYLUnx8vKYe6trpuXPnvD93dXWFpE2RhD0cKcXAkVJBBa6kpAQLFixAdHQ0EhMTsW7dOr/LMz09PXA6nYiPj8fkyZOxcePGMXHrNAUmqDFcXV0dnE4nFixYgJ8/f+Lw4cNYvnw5mpqaMGnSJADAvn37cOfOHVRXV8Nms6GgoAAbNmzA48ePQ/IfCDXf5YjM5uC+FJ48eaJncyJeUIG7e/eupq6srERiYiIaGxuxZMkSuFwuXLhwAVeuXMGyZcsADHxgs2fPRn19PRYuXOj3Ozlt/tgyojHcr78Mj4uLAwA0Njbix48fmmnzZ82aBYfDwWnzCcAIAufxeLB3714sXrwYc+bMATAwbb7FYkFsbKxmX06bT78M+zyc0+nEy5cv/ebHCJbRps33vd/Nd1lw3/NufX19mrq8vFxT84BJa1g9XEFBAW7fvo379+9j2rRp3seTk5PR19fnt74Ap82nX4IKnIigoKAAN27cQG1tLdLS0jTPp6enY/z48ZqJ9Zqbm/Hx40dOm08AgvxKdTqduHLlCm7duoXo6GjvuMxms2HChAmw2WzYvn07CgsLERcXh5iYGOzZswdZWVl/PEKlscckg01J7ruzyfTHxysqKrz3evX09GD//v24evUqent7kZeXh3PnzgX8lep2u4dcejuUcnJyNPU///yjqX3Pw7W0tGjqGTNmhKRdkcDlciEmJmbQfYLq4QLJZlRUFMrLy/0Gz0QAr6WSYgwcKcXAkVIMHCnFwJFSvMXcx5s3bzS17+1F2dnZKpsz6rCHI6UYOFKKgSOlgrq0pUK4L23R8AVyaYs9HCnFwJFSDBwpxcCRUgwcKcXAkVIMHCnFwJFSDBwpxcCRUoYLnMGutFEQAvnsDBe4zs7OcDeBhimQz85wF+89Hg8+ffoEEYHD4UBra+uQF4Tp/9xuN1JSUpS+byKCzs5O2O32IefPM9wdv2azGdOmTfPOExcTE8PADYPq9y3QO3wM95VKoxsDR0oZNnBWqxXHjh0z1NxxkcDo75vhDhpodDNsD0ejEwNHSjFwpBQDR0oxcKSUYQNXXl6O1NRUREVFITMzE8+fPw93kwwjotc8EwOqqqoSi8UiFy9elFevXsmOHTskNjZW2tvbw900Q8jLy5OKigp5+fKlvHjxQlatWiUOh0O6urq8++zcuVNSUlKkpqZGGhoaZOHChbJo0aIwtnqAIQOXkZEhTqfTW/f394vdbpeSkpIwtsq4vnz5IgCkrq5OREQ6Ojpk/PjxUl1d7d3n9evXAkCePn0armaKiIjhvlL7+vrQ2NioWQHGbDYjNzf3r+t1jXV6rHmmiuEC9/XrV/T39yMpKUnz+GDrdY1leq15porhbk+i4Oi15pkqhuvhEhISMG7cOL8jKq7X5S8S1zwzXOAsFgvS09M163V5PB7U1NRwva7/SCSveRbWQ5a/qKqqEqvVKpWVldLU1CT5+fkSGxsrbW1t4W6aIezatUtsNps8ePBAPn/+7N2+ffvm3Wfnzp3icDiktrZWGhoaJCsrS7KyssLY6gGGDJyISFlZmTgcDrFYLJKRkSH19fXhbpJhAPjjVlFR4d3n+/fvsnv3bpkyZYpMnDhR1q9fL58/fw5fo//D++FIKcON4Wh0Y+BIKQaOlGLgSCkGjpRi4EgpBo6UYuBIKQaOlGLgSCkGjpT6F8JCfey0+xPyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJwAAACbCAYAAACXvfL1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAALPElEQVR4nO3dXWhT5x8H8G9TTVqxpqu1rcEGuxcQFHfRrbFOpoNicbDNzYvtQuamWNTocCIbvg6co+BufMExmGiVbXR2TGW9cJSqFcEqLQjWSvGlYKSmm4wmtWqrye9/4d+zPqeaNvbkyUnz/cCB/HJOkqfpl5PnPDl5ToaICIg0cSS7AZReGDjSioEjrRg40oqBI60YONKKgSOtGDjSioEjrRg40iphgdu/fz+mT5+OrKws+Hw+XLx4MVEvRSkkIxHfpf7222/49NNP8eOPP8Ln82H37t2oq6tDR0cHCgoKYj42Go2iq6sLOTk5yMjIsLpplAAigt7eXng8Hjgcw+zDJAHKysrE7/cbdSQSEY/HI9XV1cM+NhAICAAuKbgEAoFh/7+Wf6QODAygtbUVFRUVxn0OhwMVFRU4f/78kO37+/sRDoeNRXjySsrKyckZdhvLA3f37l1EIhEUFhYq9xcWFiIYDA7Zvrq6Gm6321i8Xq/VTSJNRtIFSvpR6qZNmxAKhYwlEAgku0mUQOOsfsL8/HxkZmaiu7tbub+7uxtFRUVDtne5XHC5XFY3g2zK8j2c0+lEaWkpGhsbjfui0SgaGxtRXl5u9ctRqhnN0ejz1NbWisvlkpqaGmlvb5eqqirJzc2VYDA47GNDoVDSj7a4vNgSCoWG/f8mJHAiIvv27ROv1ytOp1PKysqkubl5RI9j4FJ3GUngEjLwOxrhcBhutzvZzaAXEAqFMGnSpJjbJP0oldILA0daMXCkFQNHWjFwpBUDR1oxcKQVA0daMXCkFQNHWjFwpBUDR1oxcKQVA0daMXCkFQNHWjFwpJXlv9pKdz6fT6mXLl1q3J4/f76ybubMmTGfa+PGjUrd1dWl1PPmzVPqn3/+WakvXLgQu7FJwD0cacXAkVYMHGnFPtwoffzxx0q9Z88epc7Pzzdum+feOHPmjFJPmTJFqb///vuYr21+PvPjP/nkk5iPTwbu4UgrBo60YuBIK/bhhjFunPoWvfHGG0r9008/KfWECROU+uzZs8btb7/9Vll37tw5pTbPInX06FGlXrhwYcy2trS0xFxvB9zDkVYMHGkVd+DOnj2L9957Dx6PBxkZGTh+/LiyXkSwfft2TJ06FdnZ2aioqMC1a9esai+luLj7cH19fXj99dexfPlyfPTRR0PW79q1C3v37sXhw4dRUlKCbdu2obKyEu3t7cjKyrKk0ToN/i4UAA4cOBBz+4aGBqUePE4XDodjPtY8pjdcn+327dtKffjw4Zjb20HcgVu0aBEWLVr0zHUigt27d2Pr1q344IMPAABHjhxBYWEhjh8//syByP7+fvT39xv1cP8USm2W9uE6OzsRDAaVKfPdbjd8Pt8zp8wHhs5iXlxcbGWTyGYsDdzTafFHOmU+wFnM003Sx+HsNou5eaxs8+bNSm2eMPSHH35Q6q1btyp1PF2ELVu2jHhbAPjiiy+U+p9//onr8clg6R7u6bT4I50yn9KPpYErKSlBUVGRMmV+OBzGhQsXOGU+AXiBj9R79+7h+vXrRt3Z2YlLly4hLy8PXq8X69evx86dO/Haa68ZwyIejweLFy+2st2UouIOXEtLC9555x2j3rBhAwBg2bJlqKmpwVdffYW+vj5UVVWhp6cH8+bNw8mTJ207Brd9+3alNvfZBgYGlPqvv/5S6q+//lqpHzx48NzXMr8H5nE283XGzOe77dy5U6lPnDjx3Neyq7gDt2DBgphX/MvIyMCOHTuwY8eOUTWMxiZ+l0paMXCkVdLH4XTLzc1V6jVr1ii1ubtg7rPFe/Dz6quvGrd/+eUXZV1paWnMx/7+++9KvWvXrrhe2464hyOtGDjSKu0+Up1Op1IP/hnfs5i/PiooKFDqzz//XKnff/99pZ41a5Zxe+LEico688e3uTZP3dDX1xezramAezjSioEjrRg40irtLtBrHha5evWqUpunSzB/vRTv2zV4ii3zc02dOlWpzacXmdfbHS/QS7bDwJFWDBxplXbjcD09PUpt/qqqvr5eqfPy8pT6xo0bSm0+Raimpkap//33X+N2bW2tss7cRzOvH4u4hyOtGDjSioEjrdKuD2dmnlrePA43Wm+//bZx2zxtfjQaVeqbN29a+tp2xD0cacXAkVYMHGmV9n24RMvOzjZum/ts5u9lOQ5HZDEGjrRi4Egr9uESzPwzw3THPRxpxcCRVnEFrrq6Gm+++SZycnJQUFCAxYsXo6OjQ9nm4cOH8Pv9mDx5MiZOnIglS5YMmaCQ0ldcgWtqaoLf70dzczMaGhrw6NEjLFy4UPm95Jdffok///wTdXV1aGpqQldX1zOn108XlZWVxkIAZBT+/vtvASBNTU0iItLT0yPjx4+Xuro6Y5urV68KADl//vwzn+Phw4cSCoWMJRAICIAxs1RWVhpLJBJRlsePHyvLlClTlCXZbY93CYVCw2ZmVH24UCgE4L+zYltbW/Ho0SNl2vwZM2bA6/Vy2nwCMIqDhmg0ivXr1+Ott94ypjMIBoNwOp1DforHafPpqRceh/P7/WhraxtyCcZ42W3afKu9/PLLyW6CrbzQHm7t2rWor6/H6dOnMW3aNOP+oqIiDAwMDPmhCqfNp6fiCpyIYO3atTh27BhOnTqFkpISZX1paSnGjx+vTJvf0dGBW7ducdp8AhDnR6rf78evv/6KEydOICcnx+iXud1uZGdnw+12Y8WKFdiwYQPy8vIwadIkrFu3DuXl5ZgzZ05C/gBKMfEMg+A5h8OHDh0ytnnw4IGsWbNGXnrpJZkwYYJ8+OGHcufOnRG/RigUSvrhvZXLrFmzjMXMPEySDsMiaTeZjW6DJyS8fPmyss58Qqa5n5sK184ajJPZkO0wcKQVz4dLsLa2NuP2tWvXlHXmMbpXXnlFqVPtI3UkuIcjrRg40opHqRp99tlnSn3gwAGlbmpqUup169YpdXt7e0LaZRUepZLtMHCkFQNHWrEPp5G5f3P06FGlHnziKgD88ccfSm2+zJLdLoXEPhzZDgNHWjFwpBX7cElk7u989913Sr169Wqlnj17tlLbbVyOfTiyHQaOtGLgSCv24cgy7MOR7TBwpJXtAmezT3iKw0j+d7YLXG9vb7KbQC9oJP872x00RKNRdHV1QUTg9XoRCASG7YjSf8LhMIqLi7W+byKC3t5eeDweOByx92G2+xGNw+HAtGnTEA6HATwZjWfg4qf7fRvpyILtPlJpbGPgSCvbBs7lcuGbb74Z03PHJYLd3zfbHTTQ2GbbPRyNTQwcacXAkVYMHGnFwJFWtg3c/v37MX36dGRlZcHn8+HixYvJbpJtpPQ1z0Y8+a5GtbW14nQ65eDBg3LlyhVZuXKl5ObmSnd3d7KbZguVlZVy6NAhaWtrk0uXLsm7774rXq9X7t27Z2yzatUqKS4ulsbGRmlpaZE5c+bI3Llzk9jqJ2wZuLKyMvH7/UYdiUTE4/FIdXV1EltlX1Zc80wX232kDgwMoLW1VZn2wOFwoKKi4rnX60p3VlzzTBfbBe7u3buIRCIoLCxU7o91va50ZtU1z3Sx3elJFB+rrnmmi+32cPn5+cjMzBxyRMXrdQ2Vitc8s13gnE4nSktLlet1RaNRNDY28npd/yepfM2zpB6yPEdtba24XC6pqamR9vZ2qaqqktzcXAkGg8lumi2sXr1a3G63nDlzRu7cuWMs9+/fN7ZZtWqVeL1eOXXqlLS0tEh5ebmUl5cnsdVP2DJwIiL79u0Tr9crTqdTysrKpLm5OdlNsg1ouOZZovB8ONLKdn04GtsYONKKgSOtGDjSioEjrRg40oqBI60YONKKgSOtGDjSioEjrf4HnM8gSCQNzYgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJwAAACbCAYAAACXvfL1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAKDUlEQVR4nO3dXUhT/x8H8Lfr32YPOrNIGzk0KwyiLkTNiiAQrOi5my6iosgeZiUFQdETFAhd9WR0lauLMrzIqKAbe6KoRKELm2mF1MC0unAzK63t87/o1+ho6ebOvjvb3i84sO92jn7d3nzPw46fb5KICIgUMUW7A5RYGDhSioEjpRg4UoqBI6UYOFKKgSOlGDhSioEjpRg4UipigauqqkJ2djaSk5NRVFSEhoaGSP0qiiFJkfgu9fr169i4cSMuXryIoqIinD59GrW1tWhtbcXkyZOH3Nbv96OjowMpKSlISkrSu2sUASKCnp4e2Gw2mEzDjGESAYWFheJwOAJtn88nNptNKisrh93W7XYLAC4xuLjd7mE/X913qf39/WhqakJJSUngOZPJhJKSEjx9+nTQ+n19ffB6vYFFePNKzEpJSRl2Hd0D9/nzZ/h8PmRkZGiez8jIQGdn56D1KysrYbVaA4vdbte7S6RIMIdAUT9LPXjwIDweT2Bxu93R7hJF0P/0/oGTJk3CqFGj0NXVpXm+q6sLmZmZg9a3WCywWCx6d4MMSvcRzmw2Iz8/H/X19YHn/H4/6uvrUVxcrPevo1gTztnov9TU1IjFYhGn0ykul0vKysokLS1NOjs7h93W4/FE/WyLy8gWj8cz7OcbkcCJiJw7d07sdruYzWYpLCyUZ8+eBbUdAxe7SzCBi8iF33B4vV5YrdZod4NGwOPxIDU1dch1on6WSomFgSOlGDhSioEjpRg4UoqBI6V0/2or0Z05c0bT3rNnT+Bxc3Oz5rXly5dr2u/evYtcxwyCIxwpxcCRUtylhik7O1vT3rBhg6bt9/sDj2fNmqV5LS8vT9PmLpVIZwwcKcXAkVI8hgvTp0+fNO1Hjx5p2itXrlTZHcPjCEdKMXCkFANHSvEYLky9vb2adiJcSwsHRzhSioEjpRg4UorHcGFKS0vTtOfOnRudjsQIjnCkFANHSjFwpBSP4cI0duxYTTuU+nYFBQWa9qtXrzTteLymxxGOlGLgSKmQA/fo0SOsWLECNpsNSUlJqKur07wuIjh69CimTJmCMWPGoKSkBK9fv9arvxTjQj6G6+3txdy5c7FlyxasXbt20OunTp3C2bNncfnyZeTk5ODIkSMoLS2Fy+VCcnKyLp02ko6ODk3b6XRq2sePH//ntgNf6+7u1rTPnz8fRs+MKeTALV26FEuXLv3rayKC06dP4/Dhw1i1ahUA4MqVK8jIyEBdXR3Wr18/aJu+vj709fUF2l6vN9QuUQzR9Riuvb0dnZ2dmpL5VqsVRUVFfy2ZDwyuYp6VlaVnl8hgdA3c77L4wZbMB1jFPNFE/TpcvFUxP3HihKY91DFcItJ1hPtdFj/YkvmUeHQNXE5ODjIzMzUl871eL54/f86S+QRgBLvUL1++4M2bN4F2e3s7Xrx4gfT0dNjtdlRUVODkyZOYMWNG4LKIzWbD6tWr9ew3xaiQA9fY2IjFixcH2vv27QMAbNq0CU6nEwcOHEBvby/KysrQ3d2NhQsX4u7du3F5DS4Yf07n+GedkUTFsvkR9ufbO1zg9u7dq2nH2oVfls0nw2HgSKmoX4eLd3/uRg129BIVHOFIKQaOlGLgSCkGjpRi4EgpBo6UYuBIKQaOlGLgSCkGjpTiV1sRFsrtSYsWLdK0Y+1ukWBwhCOlGDhSioEjpXjHb4T5fL7A41Df6jlz5mjaLpdLlz5FCu/4JcNh4EgpBo6U4nW4CLt48WLg8fbt20PatqysTNOuqKjQo0tRxRGOlGLgSCkGjpTiMVyEDSyFn+g4wpFSDBwpFVLgKisrUVBQgJSUFEyePBmrV69Ga2urZp3v37/D4XBg4sSJGD9+PNatWzeoQCElrpC+S12yZAnWr1+PgoIC/Pz5E4cOHUJzczNcLhfGjRsHANi5cyfu3LkDp9MJq9WK8vJymEwmPHnyJKjfEW/fpf6pra1N087NzR1y/T/vpQOA6dOna9pv377Vp2M6Cea7VEgYPn78KADk4cOHIiLS3d0to0ePltra2sA6LS0tAkCePn3615/x/ft38Xg8gcXtdguAuFza2to0i8/nG3IZKDc3V7NE++8ZuHg8nmEzE9YxnMfjAQCkp6cDAJqamvDjxw9N2fy8vDzY7XaWzScAYZw0+P1+VFRUYMGCBZg9ezaAX2XzzWbzoFmSWTaffhvxdTiHw4Hm5mY8fvw4rA7EW9n8obx8+VLTnjZt2pDrx2OJ1hGNcOXl5bh9+zbu37+PqVOnBp7PzMxEf3//oDmjWDaffgspcCKC8vJy3LhxA/fu3UNOTo7m9fz8fIwePVpTNr+1tRXv379n2XwCEOIu1eFw4OrVq7h58yZSUlICx2VWqxVjxoyB1WrF1q1bsW/fPqSnpyM1NRW7d+9GcXEx5s2bF5E/gGJLSNfhkpKS/vp8dXU1Nm/eDODXhd/9+/fj2rVr6OvrQ2lpKS5cuBD0LjWer8MNnIXx1q1bQ64/8P2eOXOmph2L1+FCGuGCyWZycjKqqqpQVVUVyo+mBMHvUkkpBo6U4v1wCg38v9KWlhZNe9asWSq7ExUc4UgpBo6UYqkH0g1LPZDhMHCkFANHSjFwpBQDR0oxcKQUA0dKMXCkFANHSjFwpBQDR0oxcKQUA0dKMXCklOECZ7C7pSgEwXx2hgtcT09PtLtAIxTMZ2e4GzD9fj86OjogIrDb7XC73cPXHKMAr9eLrKwspe+biKCnpwc2m21QTbuBDPdPNCaTCVOnToXX6wUApKamMnAjoPp9C/YubcPtUim+MXCklGEDZ7FYcOzYsYSpHacXo79vhjtpoPhm2BGO4hMDR0oxcKQUA0dKMXCklGEDV1VVhezsbCQnJ6OoqAgNDQ3R7pJhxPScZ0HPc6RQTU2NmM1muXTpkrx8+VK2bdsmaWlp0tXVFe2uGUJpaalUV1dLc3OzvHjxQpYtWyZ2u12+fPkSWGfHjh2SlZUl9fX10tjYKPPmzZP58+dHsde/GDJwhYWF4nA4Am2fzyc2m00qKyuj2Cvj0mPOM1UMt0vt7+9HU1OTZr4uk8mEkpKSf87Xlej0mPNMFcMF7vPnz/D5fMjIyNA8P9R8XYlMrznPVDHc7UkUGr3mPFPFcCPcpEmTMGrUqEFnVJyva7BYnPPMcIEzm83Iz8/XzNfl9/tRX1/P+br+I7E851lUT1n+oaamRiwWizidTnG5XFJWViZpaWnS2dkZ7a4Zws6dO8VqtcqDBw/kw4cPgeXr16+BdXbs2CF2u13u3bsnjY2NUlxcLMXFxVHs9S+GDJyIyLlz58Rut4vZbJbCwkJ59uxZtLtkGPjHFODV1dWBdb59+ya7du2SCRMmyNixY2XNmjXy4cOH6HX6P7wfjpQy3DEcxTcGjpRi4EgpBo6UYuBIKQaOlGLgSCkGjpRi4EgpBo6UYuBIqf8DNfay+FTRoIQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "for i in range(9):  \n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    pyplot.imshow(train_X[i], cmap=pyplot.get_cmap('gray'))\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Gebruiker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "X_train: (60000, 28, 28)\n",
      "Y_train: (60000,)\n",
      "X_test:  (10000, 28, 28)\n",
      "Y_test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "#loading the dataset\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "\n",
    "#printing the shapes of the vectors \n",
    "print('X_train: ' + str(train_X.shape))\n",
    "print('Y_train: ' + str(train_y.shape))\n",
    "print('X_test:  '  + str(test_X.shape))\n",
    "print('Y_test:  '  + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the error margin, we will use back prop to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example loading data, replace this with your actual data loading code\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "\n",
    "# Reshape the images from (number_of_images, 28, 28) to (number_of_images, 784)\n",
    "train_X_flattened = train_X.reshape(train_X.shape[0], -1)\n",
    "test_X_flattened = test_X.reshape(test_X.shape[0], -1)\n",
    "\n",
    "# Normalize the pixel values to the range [0, 1]\n",
    "train_X_flattened = train_X_flattened / 255.0\n",
    "test_X_flattened = test_X_flattened / 255.0\n",
    "\n",
    "# Transpose the data if each column should represent an image\n",
    "train_X_flattened = train_X_flattened.T\n",
    "test_X_flattened = test_X_flattened.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 60000)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    \"\"\"\n",
    "    Initialize the parameters of a 2-layer neural network.\n",
    "    The network has 784 input features and 10 output units in each layer.\n",
    "    \n",
    "    Returns:\n",
    "    W1, b1: Weight and bias for the first layer.\n",
    "    W2, b2: Weight and bias for the second layer.\n",
    "    \"\"\"\n",
    "    W1 = np.random.normal(size=(10, 784)) * np.sqrt(1./(784))\n",
    "    #b1 = np.random.normal(size=(10, 1)) * np.sqrt(1./10)\n",
    "    b1 = np.zeros((10, 1))\n",
    "    W2 = np.random.normal(size=(10, 10)) * np.sqrt(1./20)\n",
    "    #b2 = np.random.normal(size=(10, 1)) * np.sqrt(1./(784))\n",
    "    b2 = np.zeros((10, 1))\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLU(Z):\n",
    "    \"\"\"\n",
    "    Apply the Rectified Linear Unit (ReLU) function element-wise.\n",
    "    \n",
    "    Z: Input array.\n",
    "    \n",
    "    Returns:\n",
    "    Array with ReLU applied (all negative elements set to 0).\n",
    "    \"\"\"\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "\n",
    "def LeakyReLU(Z, alpha=0.01):\n",
    "    return np.where(Z > 0, Z, alpha * Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Apply the softmax function to each column of the input array.\n",
    "    \n",
    "    Z: Input array.\n",
    "    \n",
    "    Returns:\n",
    "    Softmax applied array.\n",
    "    \"\"\"\n",
    "    Z -= np.max(Z, axis=0)  # Improve numerical stability\n",
    "    exp_Z = np.exp(Z)\n",
    "    return exp_Z / (np.sum(exp_Z, axis=0, keepdims=True) + 1e-8)\n",
    "\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    \"\"\"\n",
    "    Perform forward propagation through the network.\n",
    "    \n",
    "    W1, b1: Weight and bias for the first layer.\n",
    "    W2, b2: Weight and bias for the second layer.\n",
    "    X: Input data (each column is an input example).\n",
    "    \n",
    "    Returns:\n",
    "    Z1, A1: Pre-activation and post-activation values for the first layer.\n",
    "    Z2, A2: Pre-activation and post-activation values for the second layer.\n",
    "    \"\"\"\n",
    "    Z1 = W1.dot(X) + b1       # Linear step for layer 1\n",
    "    A1 = LeakyReLU(Z1)             # Activation step for layer 1\n",
    "    Z2 = W2.dot(A1) + b2      # Linear step for layer 2\n",
    "    A2 = softmax(Z2)          # Activation step for layer 2 (softmax)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def one_hot(Y):\n",
    "    \"\"\"\n",
    "    Convert a vector of labels to one-hot encoding.\n",
    "    \n",
    "    Y: Input array of labels.\n",
    "    \n",
    "    Returns:\n",
    "    One-hot encoded matrix.\n",
    "    \"\"\"\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def deriv_ReLU(Z):\n",
    "    \"\"\"\n",
    "    Compute the derivative of the ReLU function.\n",
    "    \n",
    "    Z: Input array.\n",
    "    \n",
    "    Returns:\n",
    "    Array with derivatives of ReLU.\n",
    "    \"\"\"\n",
    "    return Z > 0\n",
    "\n",
    "\n",
    "def deriv_LeakyReLU(Z, alpha=0.01):\n",
    "    return np.where(Z > 0, 1, alpha)\n",
    "\n",
    "def back_prop(Z1, A1, Z2, A2, W2, X, Y):\n",
    "    \"\"\"\n",
    "    Perform backpropagation to compute gradients.\n",
    "    \n",
    "    Z1, A1: Pre-activation and post-activation values for the first layer.\n",
    "    Z2, A2: Pre-activation and post-activation values for the second layer.\n",
    "    W2: Weight matrix for the second layer.\n",
    "    X: Input data.\n",
    "    Y: True labels.\n",
    "    \n",
    "    Returns:\n",
    "    dW1, db1: Gradients of loss with respect to W1 and b1.\n",
    "    dW2, db2: Gradients of loss with respect to W2 and b2.\n",
    "    \"\"\"\n",
    "    m = Y.size\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y        # Derivative of loss with respect to Z2\n",
    "    dW2 = 1 / m * dZ2.dot(A1.T) # Gradient of loss with respect to W2\n",
    "    db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True) # Gradient of loss with respect to b2; corrected axis\n",
    "    dZ1 = W2.T.dot(dZ2) * deriv_LeakyReLU(Z1) # Derivative of loss with respect to Z1\n",
    "    dW1 = 1 / m * dZ1.dot(X.T)  # Gradient of loss with respect to W1\n",
    "    db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True) # Gradient of loss with respect to b1; corrected axis\n",
    "\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    \"\"\"\n",
    "    Update the parameters of the network using gradient descent.\n",
    "    \n",
    "    W1, b1: Current weight and bias for the first layer.\n",
    "    W2, b2: Current weight and bias for the second layer.\n",
    "    dW1, db1: Gradients for W1 and b1.\n",
    "    dW2, db2: Gradients for W2 and b2.\n",
    "    alpha: Learning rate.\n",
    "    \n",
    "    Returns:\n",
    "    Updated W1, b1, W2, b2.\n",
    "    \"\"\"\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * np.reshape(db1, (10, 1))\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * np.reshape(db2, (10, 1))\n",
    "\n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    \"\"\"\n",
    "    Generate predictions from the output layer's activations.\n",
    "\n",
    "    A2: The output activations from the last layer of the neural network, \n",
    "        where each column corresponds to the activations for a given input example.\n",
    "\n",
    "    Returns:\n",
    "    An array of predicted class labels for each input example.\n",
    "    \"\"\"\n",
    "    return np.argmax(A2, 0)  # np.argmax returns the indices of the max values along axis 0.\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of predictions against the true labels.\n",
    "\n",
    "    predictions: An array of predicted class labels.\n",
    "    Y: The true labels.\n",
    "\n",
    "    Returns:\n",
    "    The accuracy as a float.\n",
    "    \"\"\"\n",
    "    print(predictions, Y)  # Optionally print the predictions and true labels for inspection.\n",
    "    return np.sum(predictions == Y) / Y.size  # Calculate the proportion of correct predictions.\n",
    "\n",
    "def compute_loss(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    loss = -np.sum(Y * np.log(A2)) / m\n",
    "    return loss\n",
    "\n",
    "def gradient_descent(X, Y, iterations, alpha, W1=None, b1=None, W2=None, b2=None):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to train the neural network.\n",
    "\n",
    "    X: Input data, where each column is an input example (e.g., a flattened image).\n",
    "    Y: True labels for the input data.\n",
    "    iterations: The number of iterations to train the network.\n",
    "    alpha: The learning rate.\n",
    "\n",
    "    Returns:\n",
    "    The final weights and biases after training.\n",
    "    \"\"\"\n",
    "    if W1 is None or b1 is None or W2 is None or b2 is None:\n",
    "        W1, b1, W2, b2 = init_params()  # Initialize parameters.\n",
    "        \n",
    "    for i in range(iterations):\n",
    "        # Forward propagation.\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "\n",
    "        # Backward propagation.\n",
    "        dW1, db1, dW2, db2 = back_prop(Z1, A1, Z2, A2, W2, X, Y)\n",
    "\n",
    "        # Update parameters.\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "\n",
    "        # Print accuracy every 50 iterations.\n",
    "        if i % 50 == 0:\n",
    "            loss = compute_loss(A2, one_hot(Y))\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(\"Iteration: \", i, \"Loss: \", loss, \"Accuracy: \", get_accuracy(predictions, Y))\n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "iterations = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "[2 0 9 ... 9 0 2] [5 0 4 ... 5 6 8]\n",
      "Iteration:  0 Loss:  2.2881517273087946 Accuracy:  0.1333\n",
      "Iteration:  50\n",
      "[3 0 3 ... 9 0 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  50 Loss:  1.7508805623358452 Accuracy:  0.5039\n",
      "Iteration:  100\n",
      "[3 0 4 ... 9 0 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  100 Loss:  1.2329154876591089 Accuracy:  0.7041166666666666\n",
      "Iteration:  150\n",
      "[3 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  150 Loss:  0.8583140430423932 Accuracy:  0.8090333333333334\n",
      "Iteration:  200\n",
      "[3 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  200 Loss:  0.6665745349034801 Accuracy:  0.8394666666666667\n",
      "Iteration:  250\n",
      "[3 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  250 Loss:  0.5701338941059922 Accuracy:  0.8559666666666667\n",
      "Iteration:  300\n",
      "[3 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  300 Loss:  0.5124126652323541 Accuracy:  0.8657\n",
      "Iteration:  350\n",
      "[3 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  350 Loss:  0.4734855105529864 Accuracy:  0.8740166666666667\n",
      "Iteration:  400\n",
      "[3 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  400 Loss:  0.44516683975387894 Accuracy:  0.8793166666666666\n",
      "Iteration:  450\n",
      "[3 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  450 Loss:  0.4234945778272493 Accuracy:  0.8841333333333333\n",
      "Iteration:  500\n",
      "[3 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  500 Loss:  0.40626399412226105 Accuracy:  0.8885833333333333\n",
      "Iteration:  550\n",
      "[3 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  550 Loss:  0.39219496209335586 Accuracy:  0.8914\n",
      "Iteration:  600\n",
      "[3 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  600 Loss:  0.38047664217965815 Accuracy:  0.8937166666666667\n",
      "Iteration:  650\n",
      "[3 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  650 Loss:  0.37055245515277413 Accuracy:  0.89595\n",
      "Iteration:  700\n",
      "[3 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  700 Loss:  0.36202482980685685 Accuracy:  0.8984666666666666\n",
      "Iteration:  750\n",
      "[3 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  750 Loss:  0.3546170736689824 Accuracy:  0.9003\n",
      "Iteration:  800\n",
      "[3 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  800 Loss:  0.34811858389277334 Accuracy:  0.9016333333333333\n",
      "Iteration:  850\n",
      "[3 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  850 Loss:  0.34236502701045274 Accuracy:  0.9027833333333334\n",
      "Iteration:  900\n",
      "[3 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  900 Loss:  0.33723358893153654 Accuracy:  0.90395\n",
      "Iteration:  950\n",
      "[3 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  950 Loss:  0.3326115326352874 Accuracy:  0.9049666666666667\n",
      "Iteration:  1000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1000 Loss:  0.3284344607951646 Accuracy:  0.9059666666666667\n",
      "Iteration:  1050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1050 Loss:  0.3246373937810277 Accuracy:  0.9068333333333334\n",
      "Iteration:  1100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1100 Loss:  0.321163702110094 Accuracy:  0.9079166666666667\n",
      "Iteration:  1150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1150 Loss:  0.31797293276486993 Accuracy:  0.9088666666666667\n",
      "Iteration:  1200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1200 Loss:  0.31502472461842845 Accuracy:  0.9095333333333333\n",
      "Iteration:  1250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1250 Loss:  0.3122864798665663 Accuracy:  0.9100833333333334\n",
      "Iteration:  1300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1300 Loss:  0.30973651176683253 Accuracy:  0.91085\n",
      "Iteration:  1350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1350 Loss:  0.30735142669088755 Accuracy:  0.91145\n",
      "Iteration:  1400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1400 Loss:  0.3051162039262056 Accuracy:  0.9120333333333334\n",
      "Iteration:  1450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1450 Loss:  0.30301556018963727 Accuracy:  0.9127166666666666\n",
      "Iteration:  1500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1500 Loss:  0.3010386757274251 Accuracy:  0.9131666666666667\n",
      "Iteration:  1550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1550 Loss:  0.29917133319373923 Accuracy:  0.9139166666666667\n",
      "Iteration:  1600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1600 Loss:  0.29740237646291895 Accuracy:  0.91455\n",
      "Iteration:  1650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1650 Loss:  0.29571854476775783 Accuracy:  0.9153166666666667\n",
      "Iteration:  1700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1700 Loss:  0.2941104467861024 Accuracy:  0.9156166666666666\n",
      "Iteration:  1750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1750 Loss:  0.29257857030132356 Accuracy:  0.91615\n",
      "Iteration:  1800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1800 Loss:  0.29111679613395497 Accuracy:  0.9167333333333333\n",
      "Iteration:  1850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1850 Loss:  0.2897179633552255 Accuracy:  0.9171833333333334\n",
      "Iteration:  1900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1900 Loss:  0.2883786710193749 Accuracy:  0.91765\n",
      "Iteration:  1950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1950 Loss:  0.2870907411903593 Accuracy:  0.918\n",
      "Iteration:  2000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2000 Loss:  0.28585146253807453 Accuracy:  0.91845\n",
      "Iteration:  2050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2050 Loss:  0.28466124117308234 Accuracy:  0.91885\n",
      "Iteration:  2100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2100 Loss:  0.2835124178656014 Accuracy:  0.9192\n",
      "Iteration:  2150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2150 Loss:  0.28240432991558345 Accuracy:  0.9194166666666667\n",
      "Iteration:  2200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2200 Loss:  0.2813326916771444 Accuracy:  0.9196166666666666\n",
      "Iteration:  2250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2250 Loss:  0.2802954712503602 Accuracy:  0.9199666666666667\n",
      "Iteration:  2300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2300 Loss:  0.2792928680042649 Accuracy:  0.9202833333333333\n",
      "Iteration:  2350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2350 Loss:  0.27832150644176484 Accuracy:  0.9203833333333333\n",
      "Iteration:  2400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2400 Loss:  0.2773798756729774 Accuracy:  0.9207\n",
      "Iteration:  2450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2450 Loss:  0.2764671894186883 Accuracy:  0.92105\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(train_X_flattened, train_y, iterations, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_model_version(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        return 0  # No models saved yet\n",
    "\n",
    "    model_files = [f for f in os.listdir(directory) if re.match(r'model_v\\d+\\.pkl', f)]\n",
    "    if not model_files:\n",
    "        return 0  # No versioned model files found\n",
    "\n",
    "    # Extract version numbers and find the maximum\n",
    "    versions = [int(re.search(r'(\\d+)', f).group()) for f in model_files]\n",
    "    return max(versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(W1, b1, W2, b2, directory='models'):\n",
    "    latest_version = get_latest_model_version(directory)\n",
    "    next_version = latest_version + 1\n",
    "    filename = f'model_v{next_version}.pkl'\n",
    "    filepath = os.path.join(directory, filename)\n",
    "\n",
    "    model_params = {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2\n",
    "    }\n",
    "\n",
    "    with open(filepath, 'wb') as file:\n",
    "        pickle.dump(model_params, file)\n",
    "    print(f\"Model saved as {filename} in {directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as model_v4.pkl in models\n"
     ]
    }
   ],
   "source": [
    "save_model(W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(directory='models'):\n",
    "    latest_version = get_latest_model_version(directory)\n",
    "    if latest_version == 0:\n",
    "        raise FileNotFoundError(\"No saved model found in the directory.\")\n",
    "\n",
    "    filename = f'model_v{latest_version}.pkl'\n",
    "    filepath = os.path.join(directory, filename)\n",
    "\n",
    "    with open(filepath, 'rb') as file:\n",
    "        model_params = pickle.load(file)\n",
    "        print(f\"Loaded model version {latest_version} from {filepath}\")\n",
    "        return model_params['W1'], model_params['b1'], model_params['W2'], model_params['b2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model version 2 from models\\model_v2.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "W1, b1, W2, b2 = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conitnue Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "iterations = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  0 Loss:  0.24193142732363712 Accuracy:  0.93155\n",
      "Iteration:  50\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  50 Loss:  0.24186586450269457 Accuracy:  0.9316333333333333\n",
      "Iteration:  100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  100 Loss:  0.24180037981818112 Accuracy:  0.9316666666666666\n",
      "Iteration:  150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  150 Loss:  0.24173506594031402 Accuracy:  0.9317\n",
      "Iteration:  200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  200 Loss:  0.24166989467604955 Accuracy:  0.9317166666666666\n",
      "Iteration:  250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  250 Loss:  0.24160480790983926 Accuracy:  0.9317333333333333\n",
      "Iteration:  300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  300 Loss:  0.24153992669699198 Accuracy:  0.9317666666666666\n",
      "Iteration:  350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  350 Loss:  0.2414752646254678 Accuracy:  0.9317666666666666\n",
      "Iteration:  400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  400 Loss:  0.24141068629326862 Accuracy:  0.93175\n",
      "Iteration:  450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  450 Loss:  0.24134618264278865 Accuracy:  0.93175\n",
      "Iteration:  500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  500 Loss:  0.24128174285359663 Accuracy:  0.9318166666666666\n",
      "Iteration:  550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  550 Loss:  0.24121746407643913 Accuracy:  0.9318166666666666\n",
      "Iteration:  600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  600 Loss:  0.24115333552488927 Accuracy:  0.9318666666666666\n",
      "Iteration:  650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  650 Loss:  0.24108937159755628 Accuracy:  0.9319333333333333\n",
      "Iteration:  700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  700 Loss:  0.2410255725854508 Accuracy:  0.9319333333333333\n",
      "Iteration:  750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  750 Loss:  0.24096194355436162 Accuracy:  0.9319333333333333\n",
      "Iteration:  800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  800 Loss:  0.24089839632995702 Accuracy:  0.9319333333333333\n",
      "Iteration:  850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  850 Loss:  0.24083487956554658 Accuracy:  0.9319166666666666\n",
      "Iteration:  900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  900 Loss:  0.24077141390173273 Accuracy:  0.93195\n",
      "Iteration:  950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  950 Loss:  0.2407080119074419 Accuracy:  0.9319166666666666\n",
      "Iteration:  1000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1000 Loss:  0.24064463720185425 Accuracy:  0.9319666666666667\n",
      "Iteration:  1050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1050 Loss:  0.24058132498114476 Accuracy:  0.9319833333333334\n",
      "Iteration:  1100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1100 Loss:  0.24051811434303522 Accuracy:  0.9319833333333334\n",
      "Iteration:  1150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1150 Loss:  0.24045500990514762 Accuracy:  0.9320666666666667\n",
      "Iteration:  1200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1200 Loss:  0.24039180585678807 Accuracy:  0.9321\n",
      "Iteration:  1250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1250 Loss:  0.24032862319828358 Accuracy:  0.9321166666666667\n",
      "Iteration:  1300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1300 Loss:  0.24026543380782994 Accuracy:  0.9321333333333334\n",
      "Iteration:  1350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1350 Loss:  0.24020234456197145 Accuracy:  0.9321833333333334\n",
      "Iteration:  1400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1400 Loss:  0.24013942944899386 Accuracy:  0.9321833333333334\n",
      "Iteration:  1450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1450 Loss:  0.24007657356847065 Accuracy:  0.9322333333333334\n",
      "Iteration:  1500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1500 Loss:  0.24001381377844552 Accuracy:  0.9322333333333334\n",
      "Iteration:  1550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1550 Loss:  0.23995124668208928 Accuracy:  0.93225\n",
      "Iteration:  1600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1600 Loss:  0.23988862591970286 Accuracy:  0.93225\n",
      "Iteration:  1650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1650 Loss:  0.23982606328891035 Accuracy:  0.9322666666666667\n",
      "Iteration:  1700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1700 Loss:  0.2397636654266091 Accuracy:  0.9323166666666667\n",
      "Iteration:  1750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1750 Loss:  0.23970139404057228 Accuracy:  0.9322833333333334\n",
      "Iteration:  1800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1800 Loss:  0.23963927280198005 Accuracy:  0.9323\n",
      "Iteration:  1850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1850 Loss:  0.2395773109870708 Accuracy:  0.9322666666666667\n",
      "Iteration:  1900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1900 Loss:  0.23951540489444487 Accuracy:  0.9323166666666667\n",
      "Iteration:  1950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  1950 Loss:  0.23945357772420847 Accuracy:  0.93235\n",
      "Iteration:  2000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2000 Loss:  0.23939186632235854 Accuracy:  0.9324\n",
      "Iteration:  2050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2050 Loss:  0.23933033355269748 Accuracy:  0.9324666666666667\n",
      "Iteration:  2100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2100 Loss:  0.23926892458730617 Accuracy:  0.9325333333333333\n",
      "Iteration:  2150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2150 Loss:  0.239207623963262 Accuracy:  0.9325333333333333\n",
      "Iteration:  2200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2200 Loss:  0.23914650951077315 Accuracy:  0.93255\n",
      "Iteration:  2250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2250 Loss:  0.23908548356623305 Accuracy:  0.9325666666666667\n",
      "Iteration:  2300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2300 Loss:  0.23902453916585523 Accuracy:  0.9325833333333333\n",
      "Iteration:  2350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2350 Loss:  0.2389636037173453 Accuracy:  0.9326166666666666\n",
      "Iteration:  2400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2400 Loss:  0.2389027365607364 Accuracy:  0.9326166666666666\n",
      "Iteration:  2450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2450 Loss:  0.23884192884521727 Accuracy:  0.93265\n",
      "Iteration:  2500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2500 Loss:  0.23878122256938142 Accuracy:  0.9326666666666666\n",
      "Iteration:  2550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2550 Loss:  0.23872054430361214 Accuracy:  0.93265\n",
      "Iteration:  2600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2600 Loss:  0.23865990577682988 Accuracy:  0.9327\n",
      "Iteration:  2650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2650 Loss:  0.2385994242405085 Accuracy:  0.9327333333333333\n",
      "Iteration:  2700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2700 Loss:  0.23853909080074612 Accuracy:  0.9327333333333333\n",
      "Iteration:  2750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2750 Loss:  0.23847891006317662 Accuracy:  0.9327833333333333\n",
      "Iteration:  2800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2800 Loss:  0.23841884390715964 Accuracy:  0.9327833333333333\n",
      "Iteration:  2850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2850 Loss:  0.2383588322755306 Accuracy:  0.9327833333333333\n",
      "Iteration:  2900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2900 Loss:  0.2382988609365198 Accuracy:  0.9328\n",
      "Iteration:  2950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  2950 Loss:  0.23823894926246464 Accuracy:  0.9328666666666666\n",
      "Iteration:  3000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3000 Loss:  0.23817908093200058 Accuracy:  0.9329166666666666\n",
      "Iteration:  3050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3050 Loss:  0.23811933819947684 Accuracy:  0.9329166666666666\n",
      "Iteration:  3100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3100 Loss:  0.23805980276835165 Accuracy:  0.9329666666666667\n",
      "Iteration:  3150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3150 Loss:  0.23800037512972286 Accuracy:  0.9329666666666667\n",
      "Iteration:  3200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3200 Loss:  0.2379411346497177 Accuracy:  0.93295\n",
      "Iteration:  3250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3250 Loss:  0.23788207397234976 Accuracy:  0.9329666666666667\n",
      "Iteration:  3300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3300 Loss:  0.23782315740249885 Accuracy:  0.9329666666666667\n",
      "Iteration:  3350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3350 Loss:  0.23776439417764036 Accuracy:  0.933\n",
      "Iteration:  3400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3400 Loss:  0.2377057564161309 Accuracy:  0.9330166666666667\n",
      "Iteration:  3450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3450 Loss:  0.23764706170988514 Accuracy:  0.9330333333333334\n",
      "Iteration:  3500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3500 Loss:  0.23758853687121625 Accuracy:  0.9330333333333334\n",
      "Iteration:  3550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3550 Loss:  0.23753016458739507 Accuracy:  0.9330666666666667\n",
      "Iteration:  3600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3600 Loss:  0.23747182970607686 Accuracy:  0.9331166666666667\n",
      "Iteration:  3650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3650 Loss:  0.2374136174340265 Accuracy:  0.9331666666666667\n",
      "Iteration:  3700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3700 Loss:  0.23735550945372588 Accuracy:  0.9331333333333334\n",
      "Iteration:  3750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3750 Loss:  0.2372975246784026 Accuracy:  0.9331166666666667\n",
      "Iteration:  3800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3800 Loss:  0.2372396689345787 Accuracy:  0.93315\n",
      "Iteration:  3850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3850 Loss:  0.2371819568168305 Accuracy:  0.9331666666666667\n",
      "Iteration:  3900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3900 Loss:  0.2371243328083954 Accuracy:  0.9332166666666667\n",
      "Iteration:  3950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  3950 Loss:  0.23706681123883788 Accuracy:  0.93325\n",
      "Iteration:  4000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4000 Loss:  0.23700944432163645 Accuracy:  0.93325\n",
      "Iteration:  4050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4050 Loss:  0.23695220574870096 Accuracy:  0.93325\n",
      "Iteration:  4100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4100 Loss:  0.23689497235676601 Accuracy:  0.9333\n",
      "Iteration:  4150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4150 Loss:  0.23683781625676106 Accuracy:  0.9332833333333334\n",
      "Iteration:  4200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4200 Loss:  0.23678079952438255 Accuracy:  0.9333166666666667\n",
      "Iteration:  4250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4250 Loss:  0.23672392912270762 Accuracy:  0.9333666666666667\n",
      "Iteration:  4300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4300 Loss:  0.23666718581642524 Accuracy:  0.9333833333333333\n",
      "Iteration:  4350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4350 Loss:  0.23661056421903115 Accuracy:  0.9333666666666667\n",
      "Iteration:  4400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4400 Loss:  0.23655399501194674 Accuracy:  0.9334333333333333\n",
      "Iteration:  4450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4450 Loss:  0.23649748602934942 Accuracy:  0.9334333333333333\n",
      "Iteration:  4500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4500 Loss:  0.23644098600486418 Accuracy:  0.93345\n",
      "Iteration:  4550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4550 Loss:  0.2363846585696093 Accuracy:  0.9335\n",
      "Iteration:  4600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4600 Loss:  0.23632844628014818 Accuracy:  0.9334666666666667\n",
      "Iteration:  4650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4650 Loss:  0.23627236728051737 Accuracy:  0.9335166666666667\n",
      "Iteration:  4700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4700 Loss:  0.2362164214348128 Accuracy:  0.9335333333333333\n",
      "Iteration:  4750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4750 Loss:  0.23616061061170915 Accuracy:  0.9335\n",
      "Iteration:  4800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4800 Loss:  0.2361049188072817 Accuracy:  0.9335333333333333\n",
      "Iteration:  4850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4850 Loss:  0.2360493548184256 Accuracy:  0.9335333333333333\n",
      "Iteration:  4900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4900 Loss:  0.23599392794361265 Accuracy:  0.9335166666666667\n",
      "Iteration:  4950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  4950 Loss:  0.23593858802911558 Accuracy:  0.9335166666666667\n",
      "Iteration:  5000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5000 Loss:  0.23588328466754355 Accuracy:  0.93355\n",
      "Iteration:  5050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5050 Loss:  0.2358280620763396 Accuracy:  0.9335666666666667\n",
      "Iteration:  5100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5100 Loss:  0.23577287134630692 Accuracy:  0.9335666666666667\n",
      "Iteration:  5150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5150 Loss:  0.23571779123083222 Accuracy:  0.9336\n",
      "Iteration:  5200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5200 Loss:  0.23566278136148697 Accuracy:  0.9335833333333333\n",
      "Iteration:  5250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5250 Loss:  0.23560782187466642 Accuracy:  0.9336166666666667\n",
      "Iteration:  5300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5300 Loss:  0.2355529483669318 Accuracy:  0.9336333333333333\n",
      "Iteration:  5350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5350 Loss:  0.23549818398935227 Accuracy:  0.9336166666666667\n",
      "Iteration:  5400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5400 Loss:  0.23544350289326646 Accuracy:  0.9336333333333333\n",
      "Iteration:  5450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5450 Loss:  0.2353888335912155 Accuracy:  0.93365\n",
      "Iteration:  5500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5500 Loss:  0.23533430833119545 Accuracy:  0.9336833333333333\n",
      "Iteration:  5550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5550 Loss:  0.23527991280192184 Accuracy:  0.9337\n",
      "Iteration:  5600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5600 Loss:  0.23522559733237247 Accuracy:  0.9337166666666666\n",
      "Iteration:  5650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5650 Loss:  0.2351713167153678 Accuracy:  0.9337333333333333\n",
      "Iteration:  5700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5700 Loss:  0.23511710296164198 Accuracy:  0.9337333333333333\n",
      "Iteration:  5750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5750 Loss:  0.23506295134873664 Accuracy:  0.9337833333333333\n",
      "Iteration:  5800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5800 Loss:  0.23500893734056824 Accuracy:  0.9337833333333333\n",
      "Iteration:  5850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5850 Loss:  0.23495499772710648 Accuracy:  0.9337833333333333\n",
      "Iteration:  5900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5900 Loss:  0.23490118942083923 Accuracy:  0.9337833333333333\n",
      "Iteration:  5950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  5950 Loss:  0.23484746394956574 Accuracy:  0.93375\n",
      "Iteration:  6000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6000 Loss:  0.23479380685289064 Accuracy:  0.93375\n",
      "Iteration:  6050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6050 Loss:  0.23474025167234186 Accuracy:  0.9337333333333333\n",
      "Iteration:  6100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6100 Loss:  0.23468677220129028 Accuracy:  0.9337666666666666\n",
      "Iteration:  6150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6150 Loss:  0.23463347930555897 Accuracy:  0.9338\n",
      "Iteration:  6200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6200 Loss:  0.23458023526076288 Accuracy:  0.9338333333333333\n",
      "Iteration:  6250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6250 Loss:  0.23452696816040428 Accuracy:  0.9338666666666666\n",
      "Iteration:  6300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6300 Loss:  0.2344736298802257 Accuracy:  0.9338666666666666\n",
      "Iteration:  6350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6350 Loss:  0.2344203247412649 Accuracy:  0.9338833333333333\n",
      "Iteration:  6400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6400 Loss:  0.23436709882814658 Accuracy:  0.9338833333333333\n",
      "Iteration:  6450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6450 Loss:  0.2343140115865546 Accuracy:  0.9339166666666666\n",
      "Iteration:  6500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6500 Loss:  0.23426098281164962 Accuracy:  0.9339166666666666\n",
      "Iteration:  6550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6550 Loss:  0.23420798982404725 Accuracy:  0.9339166666666666\n",
      "Iteration:  6600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6600 Loss:  0.23415511895759664 Accuracy:  0.9339333333333333\n",
      "Iteration:  6650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6650 Loss:  0.23410234883807987 Accuracy:  0.934\n",
      "Iteration:  6700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6700 Loss:  0.23404967652476802 Accuracy:  0.9340166666666667\n",
      "Iteration:  6750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6750 Loss:  0.23399707924371566 Accuracy:  0.93405\n",
      "Iteration:  6800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6800 Loss:  0.2339445021734308 Accuracy:  0.93405\n",
      "Iteration:  6850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6850 Loss:  0.23389188870027453 Accuracy:  0.9340833333333334\n",
      "Iteration:  6900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6900 Loss:  0.233839352116051 Accuracy:  0.9341333333333334\n",
      "Iteration:  6950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  6950 Loss:  0.2337869129517358 Accuracy:  0.93415\n",
      "Iteration:  7000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7000 Loss:  0.23373456307443524 Accuracy:  0.9341666666666667\n",
      "Iteration:  7050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7050 Loss:  0.23368229100521315 Accuracy:  0.9341833333333334\n",
      "Iteration:  7100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7100 Loss:  0.23363007284893808 Accuracy:  0.9341833333333334\n",
      "Iteration:  7150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7150 Loss:  0.23357790397354508 Accuracy:  0.9341833333333334\n",
      "Iteration:  7200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7200 Loss:  0.23352577995175125 Accuracy:  0.9342\n",
      "Iteration:  7250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7250 Loss:  0.23347372426580068 Accuracy:  0.9342\n",
      "Iteration:  7300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7300 Loss:  0.23342174730188356 Accuracy:  0.9342166666666667\n",
      "Iteration:  7350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7350 Loss:  0.2333698313304709 Accuracy:  0.9342833333333334\n",
      "Iteration:  7400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7400 Loss:  0.23331799586437058 Accuracy:  0.9343\n",
      "Iteration:  7450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7450 Loss:  0.2332662712549513 Accuracy:  0.9343\n",
      "Iteration:  7500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7500 Loss:  0.2332146377718807 Accuracy:  0.9343\n",
      "Iteration:  7550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7550 Loss:  0.23316310447155625 Accuracy:  0.9343\n",
      "Iteration:  7600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7600 Loss:  0.23311167005728356 Accuracy:  0.9343166666666667\n",
      "Iteration:  7650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7650 Loss:  0.23306026437520513 Accuracy:  0.9343\n",
      "Iteration:  7700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7700 Loss:  0.23300894579881004 Accuracy:  0.93435\n",
      "Iteration:  7750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7750 Loss:  0.2329577393964168 Accuracy:  0.93435\n",
      "Iteration:  7800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7800 Loss:  0.23290664634443287 Accuracy:  0.9344\n",
      "Iteration:  7850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7850 Loss:  0.23285559010708592 Accuracy:  0.9344333333333333\n",
      "Iteration:  7900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7900 Loss:  0.23280463554643543 Accuracy:  0.93445\n",
      "Iteration:  7950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  7950 Loss:  0.23275367443862238 Accuracy:  0.9344833333333333\n",
      "Iteration:  8000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8000 Loss:  0.2327027383364842 Accuracy:  0.9345\n",
      "Iteration:  8050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8050 Loss:  0.23265190780053627 Accuracy:  0.9345\n",
      "Iteration:  8100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8100 Loss:  0.23260121079040275 Accuracy:  0.9345333333333333\n",
      "Iteration:  8150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8150 Loss:  0.23255060989477175 Accuracy:  0.93455\n",
      "Iteration:  8200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8200 Loss:  0.23250008309955947 Accuracy:  0.9346166666666667\n",
      "Iteration:  8250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8250 Loss:  0.23244962984019973 Accuracy:  0.9346166666666667\n",
      "Iteration:  8300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8300 Loss:  0.23239922650313838 Accuracy:  0.9345833333333333\n",
      "Iteration:  8350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8350 Loss:  0.23234887394197015 Accuracy:  0.9345833333333333\n",
      "Iteration:  8400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8400 Loss:  0.23229844352247925 Accuracy:  0.9346\n",
      "Iteration:  8450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8450 Loss:  0.23224795318162036 Accuracy:  0.9346333333333333\n",
      "Iteration:  8500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8500 Loss:  0.23219751024852756 Accuracy:  0.9346666666666666\n",
      "Iteration:  8550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8550 Loss:  0.23214718133987497 Accuracy:  0.9346666666666666\n",
      "Iteration:  8600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8600 Loss:  0.23209684686071513 Accuracy:  0.9346833333333333\n",
      "Iteration:  8650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8650 Loss:  0.23204668466064834 Accuracy:  0.9347166666666666\n",
      "Iteration:  8700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8700 Loss:  0.2319966096264622 Accuracy:  0.9347166666666666\n",
      "Iteration:  8750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8750 Loss:  0.23194667743960498 Accuracy:  0.9347333333333333\n",
      "Iteration:  8800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8800 Loss:  0.2318968392355456 Accuracy:  0.9347666666666666\n",
      "Iteration:  8850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8850 Loss:  0.23184700513275072 Accuracy:  0.9347833333333333\n",
      "Iteration:  8900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8900 Loss:  0.2317972257367441 Accuracy:  0.9347833333333333\n",
      "Iteration:  8950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  8950 Loss:  0.23174752533425372 Accuracy:  0.9347666666666666\n",
      "Iteration:  9000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9000 Loss:  0.23169794691915058 Accuracy:  0.9347666666666666\n",
      "Iteration:  9050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9050 Loss:  0.23164843357586212 Accuracy:  0.9347833333333333\n",
      "Iteration:  9100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9100 Loss:  0.23159900495496905 Accuracy:  0.9347833333333333\n",
      "Iteration:  9150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9150 Loss:  0.2315497466898129 Accuracy:  0.9347833333333333\n",
      "Iteration:  9200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9200 Loss:  0.2315005202544473 Accuracy:  0.9348166666666666\n",
      "Iteration:  9250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9250 Loss:  0.231451395313731 Accuracy:  0.93485\n",
      "Iteration:  9300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9300 Loss:  0.23140237295935293 Accuracy:  0.9348666666666666\n",
      "Iteration:  9350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9350 Loss:  0.2313534702826375 Accuracy:  0.93485\n",
      "Iteration:  9400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9400 Loss:  0.23130461815990636 Accuracy:  0.9348666666666666\n",
      "Iteration:  9450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9450 Loss:  0.2312558468638411 Accuracy:  0.9348666666666666\n",
      "Iteration:  9500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9500 Loss:  0.23120713373352408 Accuracy:  0.9348666666666666\n",
      "Iteration:  9550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9550 Loss:  0.23115849147115472 Accuracy:  0.9348666666666666\n",
      "Iteration:  9600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9600 Loss:  0.2311099118453826 Accuracy:  0.9348666666666666\n",
      "Iteration:  9650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9650 Loss:  0.23106143091385484 Accuracy:  0.9349\n",
      "Iteration:  9700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9700 Loss:  0.23101304109689497 Accuracy:  0.9349166666666666\n",
      "Iteration:  9750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9750 Loss:  0.23096473472402498 Accuracy:  0.9349166666666666\n",
      "Iteration:  9800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9800 Loss:  0.2309162650838079 Accuracy:  0.9348833333333333\n",
      "Iteration:  9850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9850 Loss:  0.23086785206446497 Accuracy:  0.9349333333333333\n",
      "Iteration:  9900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9900 Loss:  0.2308195155786435 Accuracy:  0.935\n",
      "Iteration:  9950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  9950 Loss:  0.2307712349788393 Accuracy:  0.9350166666666667\n",
      "Iteration:  10000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10000 Loss:  0.23072303690577686 Accuracy:  0.9350333333333334\n",
      "Iteration:  10050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10050 Loss:  0.23067493686626211 Accuracy:  0.9350166666666667\n",
      "Iteration:  10100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10100 Loss:  0.23062680534561525 Accuracy:  0.9350833333333334\n",
      "Iteration:  10150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10150 Loss:  0.23057873503308088 Accuracy:  0.9351166666666667\n",
      "Iteration:  10200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10200 Loss:  0.23053073371874389 Accuracy:  0.9351166666666667\n",
      "Iteration:  10250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10250 Loss:  0.2304827995763068 Accuracy:  0.9351166666666667\n",
      "Iteration:  10300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10300 Loss:  0.23043488392035025 Accuracy:  0.9351\n",
      "Iteration:  10350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10350 Loss:  0.2303871016260607 Accuracy:  0.9351\n",
      "Iteration:  10400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10400 Loss:  0.2303394692361748 Accuracy:  0.9351\n",
      "Iteration:  10450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10450 Loss:  0.23029191975168328 Accuracy:  0.9351166666666667\n",
      "Iteration:  10500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10500 Loss:  0.23024450361059287 Accuracy:  0.9351333333333334\n",
      "Iteration:  10550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10550 Loss:  0.23019712449792532 Accuracy:  0.9351333333333334\n",
      "Iteration:  10600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10600 Loss:  0.23014974738633504 Accuracy:  0.9351166666666667\n",
      "Iteration:  10650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10650 Loss:  0.23010237696684788 Accuracy:  0.9351166666666667\n",
      "Iteration:  10700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10700 Loss:  0.23005501931128947 Accuracy:  0.9351166666666667\n",
      "Iteration:  10750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10750 Loss:  0.2300076889569673 Accuracy:  0.9351\n",
      "Iteration:  10800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10800 Loss:  0.22996046248092 Accuracy:  0.9351666666666667\n",
      "Iteration:  10850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10850 Loss:  0.22991311453722588 Accuracy:  0.9351666666666667\n",
      "Iteration:  10900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10900 Loss:  0.2298658432767882 Accuracy:  0.9351166666666667\n",
      "Iteration:  10950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  10950 Loss:  0.2298186397976705 Accuracy:  0.9350833333333334\n",
      "Iteration:  11000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11000 Loss:  0.2297714785089562 Accuracy:  0.93505\n",
      "Iteration:  11050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11050 Loss:  0.22972436294192813 Accuracy:  0.93505\n",
      "Iteration:  11100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11100 Loss:  0.22967727626617826 Accuracy:  0.93505\n",
      "Iteration:  11150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11150 Loss:  0.2296302957861575 Accuracy:  0.9350666666666667\n",
      "Iteration:  11200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11200 Loss:  0.22958336942991592 Accuracy:  0.93515\n",
      "Iteration:  11250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11250 Loss:  0.22953649622835326 Accuracy:  0.93515\n",
      "Iteration:  11300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11300 Loss:  0.22948970325677506 Accuracy:  0.9351333333333334\n",
      "Iteration:  11350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11350 Loss:  0.22944285854651605 Accuracy:  0.9351666666666667\n",
      "Iteration:  11400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11400 Loss:  0.22939611764914114 Accuracy:  0.9351833333333334\n",
      "Iteration:  11450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11450 Loss:  0.22934940510412144 Accuracy:  0.9352\n",
      "Iteration:  11500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11500 Loss:  0.2293027013133109 Accuracy:  0.9352\n",
      "Iteration:  11550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11550 Loss:  0.22925607277578028 Accuracy:  0.9352333333333334\n",
      "Iteration:  11600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11600 Loss:  0.22920951816786211 Accuracy:  0.9352166666666667\n",
      "Iteration:  11650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11650 Loss:  0.22916309257105294 Accuracy:  0.9352333333333334\n",
      "Iteration:  11700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11700 Loss:  0.22911680271707902 Accuracy:  0.9352333333333334\n",
      "Iteration:  11750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11750 Loss:  0.22907052874047176 Accuracy:  0.9353\n",
      "Iteration:  11800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11800 Loss:  0.22902437001637116 Accuracy:  0.9353166666666667\n",
      "Iteration:  11850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11850 Loss:  0.2289783299020122 Accuracy:  0.9353166666666667\n",
      "Iteration:  11900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11900 Loss:  0.22893239686944825 Accuracy:  0.9353166666666667\n",
      "Iteration:  11950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  11950 Loss:  0.22888656785202147 Accuracy:  0.9353166666666667\n",
      "Iteration:  12000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12000 Loss:  0.2288407653581559 Accuracy:  0.9353333333333333\n",
      "Iteration:  12050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12050 Loss:  0.22879504043640222 Accuracy:  0.9353666666666667\n",
      "Iteration:  12100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12100 Loss:  0.22874936932209128 Accuracy:  0.9353666666666667\n",
      "Iteration:  12150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12150 Loss:  0.22870376041809767 Accuracy:  0.9353833333333333\n",
      "Iteration:  12200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12200 Loss:  0.22865824837969978 Accuracy:  0.9354\n",
      "Iteration:  12250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12250 Loss:  0.22861275565425243 Accuracy:  0.9354333333333333\n",
      "Iteration:  12300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12300 Loss:  0.22856731948006614 Accuracy:  0.93545\n",
      "Iteration:  12350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12350 Loss:  0.2285219805947122 Accuracy:  0.9354666666666667\n",
      "Iteration:  12400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12400 Loss:  0.2284767173744592 Accuracy:  0.9354666666666667\n",
      "Iteration:  12450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12450 Loss:  0.22843152484325938 Accuracy:  0.9354833333333333\n",
      "Iteration:  12500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12500 Loss:  0.22838636938676132 Accuracy:  0.9355166666666667\n",
      "Iteration:  12550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12550 Loss:  0.2283412975803025 Accuracy:  0.9355333333333333\n",
      "Iteration:  12600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12600 Loss:  0.2282963045377852 Accuracy:  0.9355333333333333\n",
      "Iteration:  12650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12650 Loss:  0.22825133378038223 Accuracy:  0.93555\n",
      "Iteration:  12700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12700 Loss:  0.2282063944968287 Accuracy:  0.9356166666666667\n",
      "Iteration:  12750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12750 Loss:  0.22816155657786313 Accuracy:  0.9356333333333333\n",
      "Iteration:  12800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12800 Loss:  0.22811684280805922 Accuracy:  0.93565\n",
      "Iteration:  12850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12850 Loss:  0.22807213673939625 Accuracy:  0.93565\n",
      "Iteration:  12900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12900 Loss:  0.22802741962005693 Accuracy:  0.9357\n",
      "Iteration:  12950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  12950 Loss:  0.22798281055416722 Accuracy:  0.9356833333333333\n",
      "Iteration:  13000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13000 Loss:  0.22793825880017565 Accuracy:  0.9357166666666666\n",
      "Iteration:  13050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13050 Loss:  0.22789368621825903 Accuracy:  0.9357\n",
      "Iteration:  13100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13100 Loss:  0.22784914843484233 Accuracy:  0.9357333333333333\n",
      "Iteration:  13150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13150 Loss:  0.2278046145689622 Accuracy:  0.9357333333333333\n",
      "Iteration:  13200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13200 Loss:  0.22776019600806038 Accuracy:  0.9357666666666666\n",
      "Iteration:  13250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13250 Loss:  0.22771588443685822 Accuracy:  0.9358\n",
      "Iteration:  13300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13300 Loss:  0.22767168587170775 Accuracy:  0.9358166666666666\n",
      "Iteration:  13350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13350 Loss:  0.22762759020029588 Accuracy:  0.9358\n",
      "Iteration:  13400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13400 Loss:  0.2275835583473713 Accuracy:  0.9357833333333333\n",
      "Iteration:  13450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13450 Loss:  0.2275396175700664 Accuracy:  0.9357666666666666\n",
      "Iteration:  13500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13500 Loss:  0.22749576294967394 Accuracy:  0.9357833333333333\n",
      "Iteration:  13550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13550 Loss:  0.22745194031013186 Accuracy:  0.9358\n",
      "Iteration:  13600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13600 Loss:  0.22740819154407002 Accuracy:  0.9358333333333333\n",
      "Iteration:  13650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13650 Loss:  0.22736454640667017 Accuracy:  0.93585\n",
      "Iteration:  13700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13700 Loss:  0.2273209746262064 Accuracy:  0.9358333333333333\n",
      "Iteration:  13750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13750 Loss:  0.22727750042495504 Accuracy:  0.9358333333333333\n",
      "Iteration:  13800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13800 Loss:  0.227234135996215 Accuracy:  0.93585\n",
      "Iteration:  13850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13850 Loss:  0.22719081984390527 Accuracy:  0.9358666666666666\n",
      "Iteration:  13900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13900 Loss:  0.22714756200974245 Accuracy:  0.9358666666666666\n",
      "Iteration:  13950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  13950 Loss:  0.22710436526561337 Accuracy:  0.93585\n",
      "Iteration:  14000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14000 Loss:  0.22706122545050567 Accuracy:  0.9358333333333333\n",
      "Iteration:  14050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14050 Loss:  0.22701817058005339 Accuracy:  0.93585\n",
      "Iteration:  14100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14100 Loss:  0.22697518533752978 Accuracy:  0.9358833333333333\n",
      "Iteration:  14150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14150 Loss:  0.22693221133047628 Accuracy:  0.9359\n",
      "Iteration:  14200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14200 Loss:  0.2268891894831477 Accuracy:  0.9359\n",
      "Iteration:  14250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14250 Loss:  0.22684615607726363 Accuracy:  0.9358833333333333\n",
      "Iteration:  14300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14300 Loss:  0.22680319544942723 Accuracy:  0.9359\n",
      "Iteration:  14350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14350 Loss:  0.22676034121477248 Accuracy:  0.9359166666666666\n",
      "Iteration:  14400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14400 Loss:  0.22671755908428015 Accuracy:  0.9359166666666666\n",
      "Iteration:  14450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14450 Loss:  0.22667483551553055 Accuracy:  0.9359333333333333\n",
      "Iteration:  14500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14500 Loss:  0.22663216702448508 Accuracy:  0.9359333333333333\n",
      "Iteration:  14550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14550 Loss:  0.22658959354356628 Accuracy:  0.9359833333333333\n",
      "Iteration:  14600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14600 Loss:  0.22654706317532328 Accuracy:  0.9359833333333333\n",
      "Iteration:  14650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14650 Loss:  0.22650460957153115 Accuracy:  0.9360166666666667\n",
      "Iteration:  14700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14700 Loss:  0.22646222333296445 Accuracy:  0.9360166666666667\n",
      "Iteration:  14750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14750 Loss:  0.22641990103387405 Accuracy:  0.9360333333333334\n",
      "Iteration:  14800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14800 Loss:  0.22637763773872527 Accuracy:  0.936\n",
      "Iteration:  14850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14850 Loss:  0.22633544607729592 Accuracy:  0.9360166666666667\n",
      "Iteration:  14900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14900 Loss:  0.22629332296824525 Accuracy:  0.9360166666666667\n",
      "Iteration:  14950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  14950 Loss:  0.22625126860336608 Accuracy:  0.9360166666666667\n",
      "Iteration:  15000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15000 Loss:  0.22620920854276153 Accuracy:  0.9360166666666667\n",
      "Iteration:  15050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15050 Loss:  0.2261672269377389 Accuracy:  0.9360666666666667\n",
      "Iteration:  15100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15100 Loss:  0.22612525239244285 Accuracy:  0.9360666666666667\n",
      "Iteration:  15150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15150 Loss:  0.22608332890104 Accuracy:  0.9360833333333334\n",
      "Iteration:  15200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15200 Loss:  0.22604157600827401 Accuracy:  0.9360666666666667\n",
      "Iteration:  15250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15250 Loss:  0.2259998903591793 Accuracy:  0.9360833333333334\n",
      "Iteration:  15300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15300 Loss:  0.22595828160710055 Accuracy:  0.93615\n",
      "Iteration:  15350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15350 Loss:  0.22591670675219214 Accuracy:  0.9361666666666667\n",
      "Iteration:  15400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15400 Loss:  0.22587512133980592 Accuracy:  0.9361666666666667\n",
      "Iteration:  15450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15450 Loss:  0.22583365247629938 Accuracy:  0.9361833333333334\n",
      "Iteration:  15500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15500 Loss:  0.22579229970770756 Accuracy:  0.93625\n",
      "Iteration:  15550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15550 Loss:  0.2257510445324708 Accuracy:  0.9362833333333334\n",
      "Iteration:  15600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15600 Loss:  0.22570989896692292 Accuracy:  0.9362833333333334\n",
      "Iteration:  15650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15650 Loss:  0.22566884862609426 Accuracy:  0.9362666666666667\n",
      "Iteration:  15700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15700 Loss:  0.2256278486737891 Accuracy:  0.9362666666666667\n",
      "Iteration:  15750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15750 Loss:  0.2255868917129544 Accuracy:  0.9363\n",
      "Iteration:  15800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15800 Loss:  0.22554600354575052 Accuracy:  0.9363\n",
      "Iteration:  15850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15850 Loss:  0.2255051956569385 Accuracy:  0.9363833333333333\n",
      "Iteration:  15900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15900 Loss:  0.2254644965005173 Accuracy:  0.9364333333333333\n",
      "Iteration:  15950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  15950 Loss:  0.22542389539086674 Accuracy:  0.9364333333333333\n",
      "Iteration:  16000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16000 Loss:  0.22538336930307898 Accuracy:  0.9364\n",
      "Iteration:  16050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16050 Loss:  0.2253429056799238 Accuracy:  0.9364166666666667\n",
      "Iteration:  16100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16100 Loss:  0.22530251028723874 Accuracy:  0.9364166666666667\n",
      "Iteration:  16150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16150 Loss:  0.22526217915840374 Accuracy:  0.9364333333333333\n",
      "Iteration:  16200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16200 Loss:  0.22522184700384995 Accuracy:  0.9364333333333333\n",
      "Iteration:  16250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16250 Loss:  0.2251815077028945 Accuracy:  0.93645\n",
      "Iteration:  16300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16300 Loss:  0.2251411576009711 Accuracy:  0.9364833333333333\n",
      "Iteration:  16350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16350 Loss:  0.2251007729022135 Accuracy:  0.9365\n",
      "Iteration:  16400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16400 Loss:  0.22506045808022754 Accuracy:  0.9365166666666667\n",
      "Iteration:  16450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16450 Loss:  0.22502022158067986 Accuracy:  0.9365333333333333\n",
      "Iteration:  16500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16500 Loss:  0.22498001670871667 Accuracy:  0.9365333333333333\n",
      "Iteration:  16550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16550 Loss:  0.2249399051249085 Accuracy:  0.9365833333333333\n",
      "Iteration:  16600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16600 Loss:  0.22489987833110534 Accuracy:  0.9365833333333333\n",
      "Iteration:  16650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16650 Loss:  0.2248598952758908 Accuracy:  0.9365833333333333\n",
      "Iteration:  16700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16700 Loss:  0.2248199605417979 Accuracy:  0.9365833333333333\n",
      "Iteration:  16750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16750 Loss:  0.22478007917894596 Accuracy:  0.9365833333333333\n",
      "Iteration:  16800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16800 Loss:  0.22474028893842937 Accuracy:  0.9366166666666667\n",
      "Iteration:  16850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16850 Loss:  0.2247006004085236 Accuracy:  0.9366\n",
      "Iteration:  16900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16900 Loss:  0.22466093312683966 Accuracy:  0.9366\n",
      "Iteration:  16950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  16950 Loss:  0.2246213053788794 Accuracy:  0.9366166666666667\n",
      "Iteration:  17000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17000 Loss:  0.2245817758713421 Accuracy:  0.93665\n",
      "Iteration:  17050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17050 Loss:  0.224542313412629 Accuracy:  0.93665\n",
      "Iteration:  17100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17100 Loss:  0.22450289265260875 Accuracy:  0.9366666666666666\n",
      "Iteration:  17150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17150 Loss:  0.22446350099879908 Accuracy:  0.93665\n",
      "Iteration:  17200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17200 Loss:  0.22442415580223218 Accuracy:  0.9366833333333333\n",
      "Iteration:  17250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17250 Loss:  0.2243847771927353 Accuracy:  0.9366833333333333\n",
      "Iteration:  17300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17300 Loss:  0.22434543799656437 Accuracy:  0.9367166666666666\n",
      "Iteration:  17350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17350 Loss:  0.22430618144802658 Accuracy:  0.9367833333333333\n",
      "Iteration:  17400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17400 Loss:  0.2242670057894469 Accuracy:  0.9368\n",
      "Iteration:  17450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17450 Loss:  0.2242278678407684 Accuracy:  0.9368\n",
      "Iteration:  17500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17500 Loss:  0.22418875947754402 Accuracy:  0.9367833333333333\n",
      "Iteration:  17550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17550 Loss:  0.22414957039260347 Accuracy:  0.9367833333333333\n",
      "Iteration:  17600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17600 Loss:  0.2241104752353657 Accuracy:  0.9367833333333333\n",
      "Iteration:  17650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17650 Loss:  0.22407146331831876 Accuracy:  0.9367833333333333\n",
      "Iteration:  17700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17700 Loss:  0.22403251586910408 Accuracy:  0.9368\n",
      "Iteration:  17750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17750 Loss:  0.22399361482996769 Accuracy:  0.9367833333333333\n",
      "Iteration:  17800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17800 Loss:  0.22395478404906036 Accuracy:  0.9367833333333333\n",
      "Iteration:  17850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17850 Loss:  0.22391599211331872 Accuracy:  0.9367833333333333\n",
      "Iteration:  17900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17900 Loss:  0.22387720738885966 Accuracy:  0.9367666666666666\n",
      "Iteration:  17950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  17950 Loss:  0.22383848790492722 Accuracy:  0.9367666666666666\n",
      "Iteration:  18000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18000 Loss:  0.22379986850313224 Accuracy:  0.9368166666666666\n",
      "Iteration:  18050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18050 Loss:  0.22376129539804504 Accuracy:  0.9368666666666666\n",
      "Iteration:  18100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18100 Loss:  0.2237227056760332 Accuracy:  0.9369\n",
      "Iteration:  18150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18150 Loss:  0.2236841591223294 Accuracy:  0.9368833333333333\n",
      "Iteration:  18200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18200 Loss:  0.22364570272319262 Accuracy:  0.9369\n",
      "Iteration:  18250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18250 Loss:  0.2236073405150736 Accuracy:  0.9369166666666666\n",
      "Iteration:  18300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18300 Loss:  0.22356907013137356 Accuracy:  0.9369333333333333\n",
      "Iteration:  18350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18350 Loss:  0.2235308941036422 Accuracy:  0.93695\n",
      "Iteration:  18400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18400 Loss:  0.22349278286742222 Accuracy:  0.9369833333333333\n",
      "Iteration:  18450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18450 Loss:  0.22345467426266818 Accuracy:  0.9370666666666667\n",
      "Iteration:  18500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18500 Loss:  0.22341654653961815 Accuracy:  0.9370666666666667\n",
      "Iteration:  18550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18550 Loss:  0.22337845512726695 Accuracy:  0.9370833333333334\n",
      "Iteration:  18600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18600 Loss:  0.2233403748139241 Accuracy:  0.9371\n",
      "Iteration:  18650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18650 Loss:  0.22330226002180117 Accuracy:  0.9371166666666667\n",
      "Iteration:  18700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18700 Loss:  0.2232641800320646 Accuracy:  0.9371333333333334\n",
      "Iteration:  18750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18750 Loss:  0.22322617353249594 Accuracy:  0.9371333333333334\n",
      "Iteration:  18800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18800 Loss:  0.22318824582170363 Accuracy:  0.9371333333333334\n",
      "Iteration:  18850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18850 Loss:  0.22315039521060326 Accuracy:  0.9371333333333334\n",
      "Iteration:  18900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18900 Loss:  0.2231125121712021 Accuracy:  0.9371833333333334\n",
      "Iteration:  18950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  18950 Loss:  0.22307457637594946 Accuracy:  0.9371833333333334\n",
      "Iteration:  19000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19000 Loss:  0.22303660157053629 Accuracy:  0.9372166666666667\n",
      "Iteration:  19050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19050 Loss:  0.2229986036870778 Accuracy:  0.9372166666666667\n",
      "Iteration:  19100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19100 Loss:  0.22296067025768848 Accuracy:  0.9372333333333334\n",
      "Iteration:  19150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19150 Loss:  0.22292279028347003 Accuracy:  0.9372333333333334\n",
      "Iteration:  19200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19200 Loss:  0.2228849230275086 Accuracy:  0.9372666666666667\n",
      "Iteration:  19250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19250 Loss:  0.2228470754121357 Accuracy:  0.9372666666666667\n",
      "Iteration:  19300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19300 Loss:  0.22280930655860934 Accuracy:  0.9372833333333334\n",
      "Iteration:  19350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19350 Loss:  0.2227715323386159 Accuracy:  0.9372833333333334\n",
      "Iteration:  19400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19400 Loss:  0.2227337104465232 Accuracy:  0.9372833333333334\n",
      "Iteration:  19450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19450 Loss:  0.22269596174236472 Accuracy:  0.93725\n",
      "Iteration:  19500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19500 Loss:  0.22265829229960676 Accuracy:  0.9372666666666667\n",
      "Iteration:  19550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19550 Loss:  0.2226206261801996 Accuracy:  0.9372666666666667\n",
      "Iteration:  19600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19600 Loss:  0.22258291252876947 Accuracy:  0.9372666666666667\n",
      "Iteration:  19650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19650 Loss:  0.22254527026113338 Accuracy:  0.93725\n",
      "Iteration:  19700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19700 Loss:  0.2225076955329452 Accuracy:  0.93725\n",
      "Iteration:  19750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19750 Loss:  0.22247014627004846 Accuracy:  0.9372166666666667\n",
      "Iteration:  19800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19800 Loss:  0.22243269026693385 Accuracy:  0.93725\n",
      "Iteration:  19850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19850 Loss:  0.22239531712794788 Accuracy:  0.93725\n",
      "Iteration:  19900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19900 Loss:  0.2223580239349565 Accuracy:  0.9372333333333334\n",
      "Iteration:  19950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  19950 Loss:  0.22232081085353872 Accuracy:  0.9372833333333334\n",
      "Iteration:  20000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20000 Loss:  0.22228367948277275 Accuracy:  0.93735\n",
      "Iteration:  20050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20050 Loss:  0.2222466362286909 Accuracy:  0.9373666666666667\n",
      "Iteration:  20100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20100 Loss:  0.22220965546296007 Accuracy:  0.9373666666666667\n",
      "Iteration:  20150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20150 Loss:  0.22217274120844666 Accuracy:  0.9373666666666667\n",
      "Iteration:  20200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20200 Loss:  0.22213596519347042 Accuracy:  0.9374\n",
      "Iteration:  20250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20250 Loss:  0.22209926015339423 Accuracy:  0.9374166666666667\n",
      "Iteration:  20300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20300 Loss:  0.22206260365365066 Accuracy:  0.9374\n",
      "Iteration:  20350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20350 Loss:  0.22202601206676126 Accuracy:  0.93735\n",
      "Iteration:  20400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20400 Loss:  0.22198951972822797 Accuracy:  0.9373666666666667\n",
      "Iteration:  20450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20450 Loss:  0.2219530269459405 Accuracy:  0.9373833333333333\n",
      "Iteration:  20500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20500 Loss:  0.22191656425588058 Accuracy:  0.93745\n",
      "Iteration:  20550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20550 Loss:  0.22188016428021157 Accuracy:  0.9374333333333333\n",
      "Iteration:  20600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20600 Loss:  0.2218438239761389 Accuracy:  0.9374166666666667\n",
      "Iteration:  20650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20650 Loss:  0.22180756193781706 Accuracy:  0.9374333333333333\n",
      "Iteration:  20700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20700 Loss:  0.22177131059804087 Accuracy:  0.93745\n",
      "Iteration:  20750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20750 Loss:  0.22173497543918946 Accuracy:  0.93745\n",
      "Iteration:  20800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20800 Loss:  0.22169866461155951 Accuracy:  0.9374666666666667\n",
      "Iteration:  20850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20850 Loss:  0.22166244328170392 Accuracy:  0.9374833333333333\n",
      "Iteration:  20900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20900 Loss:  0.2216263351523605 Accuracy:  0.9374833333333333\n",
      "Iteration:  20950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  20950 Loss:  0.2215903078735825 Accuracy:  0.93745\n",
      "Iteration:  21000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21000 Loss:  0.2215543612521052 Accuracy:  0.9375166666666667\n",
      "Iteration:  21050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21050 Loss:  0.2215184753729541 Accuracy:  0.9375166666666667\n",
      "Iteration:  21100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21100 Loss:  0.22148265442151796 Accuracy:  0.93755\n",
      "Iteration:  21150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21150 Loss:  0.22144693400017226 Accuracy:  0.9376\n",
      "Iteration:  21200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21200 Loss:  0.22141129645470994 Accuracy:  0.9375833333333333\n",
      "Iteration:  21250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21250 Loss:  0.22137563115902809 Accuracy:  0.9376166666666667\n",
      "Iteration:  21300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21300 Loss:  0.22133993768925578 Accuracy:  0.9376666666666666\n",
      "Iteration:  21350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21350 Loss:  0.22130426939486558 Accuracy:  0.93765\n",
      "Iteration:  21400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21400 Loss:  0.2212686774849309 Accuracy:  0.9376833333333333\n",
      "Iteration:  21450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21450 Loss:  0.22123313858673874 Accuracy:  0.9376833333333333\n",
      "Iteration:  21500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21500 Loss:  0.22119761658550402 Accuracy:  0.9376833333333333\n",
      "Iteration:  21550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21550 Loss:  0.22116213274237687 Accuracy:  0.9377333333333333\n",
      "Iteration:  21600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21600 Loss:  0.22112672196854627 Accuracy:  0.93775\n",
      "Iteration:  21650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21650 Loss:  0.22109137033959123 Accuracy:  0.9377666666666666\n",
      "Iteration:  21700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21700 Loss:  0.22105608036020455 Accuracy:  0.9377833333333333\n",
      "Iteration:  21750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21750 Loss:  0.22102077043132012 Accuracy:  0.9377833333333333\n",
      "Iteration:  21800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21800 Loss:  0.22098549225234357 Accuracy:  0.9377833333333333\n",
      "Iteration:  21850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21850 Loss:  0.22095027840015774 Accuracy:  0.9377666666666666\n",
      "Iteration:  21900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21900 Loss:  0.22091509867502124 Accuracy:  0.9377666666666666\n",
      "Iteration:  21950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  21950 Loss:  0.2208799034450857 Accuracy:  0.9377833333333333\n",
      "Iteration:  22000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22000 Loss:  0.22084478873658245 Accuracy:  0.9377833333333333\n",
      "Iteration:  22050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22050 Loss:  0.22080970554823884 Accuracy:  0.9378166666666666\n",
      "Iteration:  22100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22100 Loss:  0.22077469608593756 Accuracy:  0.9378\n",
      "Iteration:  22150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22150 Loss:  0.22073962429439994 Accuracy:  0.9377833333333333\n",
      "Iteration:  22200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22200 Loss:  0.22070459819569327 Accuracy:  0.9378\n",
      "Iteration:  22250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22250 Loss:  0.22066961038687302 Accuracy:  0.9378\n",
      "Iteration:  22300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22300 Loss:  0.22063465621396675 Accuracy:  0.9378333333333333\n",
      "Iteration:  22350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22350 Loss:  0.22059973596346852 Accuracy:  0.9378166666666666\n",
      "Iteration:  22400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22400 Loss:  0.22056486999645564 Accuracy:  0.9378166666666666\n",
      "Iteration:  22450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22450 Loss:  0.22052999619015343 Accuracy:  0.9378166666666666\n",
      "Iteration:  22500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22500 Loss:  0.2204950818449216 Accuracy:  0.9378666666666666\n",
      "Iteration:  22550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22550 Loss:  0.22046018090982988 Accuracy:  0.9378833333333333\n",
      "Iteration:  22600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22600 Loss:  0.22042533212678306 Accuracy:  0.9378666666666666\n",
      "Iteration:  22650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22650 Loss:  0.2203905367806804 Accuracy:  0.9378666666666666\n",
      "Iteration:  22700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22700 Loss:  0.2203557756975513 Accuracy:  0.9378833333333333\n",
      "Iteration:  22750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22750 Loss:  0.22032110112257008 Accuracy:  0.9379333333333333\n",
      "Iteration:  22800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22800 Loss:  0.2202864711503721 Accuracy:  0.93795\n",
      "Iteration:  22850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22850 Loss:  0.22025191471916095 Accuracy:  0.93795\n",
      "Iteration:  22900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22900 Loss:  0.22021743566755136 Accuracy:  0.9379666666666666\n",
      "Iteration:  22950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  22950 Loss:  0.22018304596798938 Accuracy:  0.9379833333333333\n",
      "Iteration:  23000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23000 Loss:  0.2201486912440479 Accuracy:  0.9380166666666667\n",
      "Iteration:  23050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23050 Loss:  0.22011436359585293 Accuracy:  0.93805\n",
      "Iteration:  23100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23100 Loss:  0.2200800855297612 Accuracy:  0.9380666666666667\n",
      "Iteration:  23150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23150 Loss:  0.22004583261606364 Accuracy:  0.9380666666666667\n",
      "Iteration:  23200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23200 Loss:  0.2200115732675601 Accuracy:  0.9380666666666667\n",
      "Iteration:  23250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23250 Loss:  0.21997733840757727 Accuracy:  0.9381666666666667\n",
      "Iteration:  23300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23300 Loss:  0.21994316943568173 Accuracy:  0.9381833333333334\n",
      "Iteration:  23350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23350 Loss:  0.2199090671576998 Accuracy:  0.9381833333333334\n",
      "Iteration:  23400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23400 Loss:  0.21987501933147724 Accuracy:  0.9382\n",
      "Iteration:  23450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23450 Loss:  0.2198409842507113 Accuracy:  0.9382\n",
      "Iteration:  23500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23500 Loss:  0.21980692587907882 Accuracy:  0.9382166666666667\n",
      "Iteration:  23550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23550 Loss:  0.21977291247387196 Accuracy:  0.93825\n",
      "Iteration:  23600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23600 Loss:  0.2197389778110146 Accuracy:  0.93825\n",
      "Iteration:  23650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23650 Loss:  0.21970507068402564 Accuracy:  0.93825\n",
      "Iteration:  23700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23700 Loss:  0.21967117501140015 Accuracy:  0.9382333333333334\n",
      "Iteration:  23750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23750 Loss:  0.21963734426247708 Accuracy:  0.9382166666666667\n",
      "Iteration:  23800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23800 Loss:  0.2196035084084643 Accuracy:  0.93825\n",
      "Iteration:  23850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23850 Loss:  0.21956965743087403 Accuracy:  0.93825\n",
      "Iteration:  23900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23900 Loss:  0.21953581859226678 Accuracy:  0.93825\n",
      "Iteration:  23950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  23950 Loss:  0.21950199366285916 Accuracy:  0.93825\n",
      "Iteration:  24000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24000 Loss:  0.21946824100039947 Accuracy:  0.93825\n",
      "Iteration:  24050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24050 Loss:  0.21943454106899052 Accuracy:  0.9382333333333334\n",
      "Iteration:  24100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24100 Loss:  0.21940088768770455 Accuracy:  0.9382\n",
      "Iteration:  24150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24150 Loss:  0.2193672541521707 Accuracy:  0.9381333333333334\n",
      "Iteration:  24200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24200 Loss:  0.21933354163569962 Accuracy:  0.9381333333333334\n",
      "Iteration:  24250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24250 Loss:  0.2192998057985158 Accuracy:  0.9381166666666667\n",
      "Iteration:  24300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24300 Loss:  0.21926609905515795 Accuracy:  0.93815\n",
      "Iteration:  24350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24350 Loss:  0.21923248818151714 Accuracy:  0.93815\n",
      "Iteration:  24400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24400 Loss:  0.21919892549823894 Accuracy:  0.93815\n",
      "Iteration:  24450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24450 Loss:  0.21916543105966527 Accuracy:  0.9381666666666667\n",
      "Iteration:  24500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24500 Loss:  0.21913199521083238 Accuracy:  0.93815\n",
      "Iteration:  24550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24550 Loss:  0.2190985312061772 Accuracy:  0.93815\n",
      "Iteration:  24600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24600 Loss:  0.2190650990401557 Accuracy:  0.9381333333333334\n",
      "Iteration:  24650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24650 Loss:  0.2190315831128366 Accuracy:  0.9381166666666667\n",
      "Iteration:  24700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24700 Loss:  0.21899808476108742 Accuracy:  0.9381333333333334\n",
      "Iteration:  24750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24750 Loss:  0.21896463698772908 Accuracy:  0.93815\n",
      "Iteration:  24800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24800 Loss:  0.2189312387771073 Accuracy:  0.93815\n",
      "Iteration:  24850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24850 Loss:  0.21889787845413788 Accuracy:  0.9381333333333334\n",
      "Iteration:  24900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24900 Loss:  0.21886458273609619 Accuracy:  0.9381166666666667\n",
      "Iteration:  24950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  24950 Loss:  0.21883127756341111 Accuracy:  0.9381333333333334\n",
      "Iteration:  25000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25000 Loss:  0.21879803234202788 Accuracy:  0.9381666666666667\n",
      "Iteration:  25050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25050 Loss:  0.2187648252501836 Accuracy:  0.9381666666666667\n",
      "Iteration:  25100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25100 Loss:  0.21873162710249852 Accuracy:  0.9381666666666667\n",
      "Iteration:  25150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25150 Loss:  0.21869844663458 Accuracy:  0.93815\n",
      "Iteration:  25200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25200 Loss:  0.2186652540639514 Accuracy:  0.93815\n",
      "Iteration:  25250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25250 Loss:  0.2186320135143887 Accuracy:  0.9382\n",
      "Iteration:  25300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25300 Loss:  0.21859885697972534 Accuracy:  0.9382333333333334\n",
      "Iteration:  25350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25350 Loss:  0.2185657364464328 Accuracy:  0.93825\n",
      "Iteration:  25400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25400 Loss:  0.21853263038390278 Accuracy:  0.9382333333333334\n",
      "Iteration:  25450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25450 Loss:  0.2184995762262774 Accuracy:  0.9382333333333334\n",
      "Iteration:  25500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25500 Loss:  0.2184665502565557 Accuracy:  0.93825\n",
      "Iteration:  25550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25550 Loss:  0.21843351564855495 Accuracy:  0.93825\n",
      "Iteration:  25600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25600 Loss:  0.21840056500213845 Accuracy:  0.9382666666666667\n",
      "Iteration:  25650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25650 Loss:  0.21836764602795714 Accuracy:  0.93825\n",
      "Iteration:  25700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25700 Loss:  0.21833468228162206 Accuracy:  0.9382666666666667\n",
      "Iteration:  25750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25750 Loss:  0.21830169846543973 Accuracy:  0.9382833333333334\n",
      "Iteration:  25800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25800 Loss:  0.21826872347208964 Accuracy:  0.9383333333333334\n",
      "Iteration:  25850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25850 Loss:  0.21823571815842288 Accuracy:  0.9383333333333334\n",
      "Iteration:  25900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25900 Loss:  0.2182027652125796 Accuracy:  0.93835\n",
      "Iteration:  25950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  25950 Loss:  0.21816982317896047 Accuracy:  0.9383166666666667\n",
      "Iteration:  26000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26000 Loss:  0.2181368151761408 Accuracy:  0.9383333333333334\n",
      "Iteration:  26050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26050 Loss:  0.21810384991804252 Accuracy:  0.93835\n",
      "Iteration:  26100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26100 Loss:  0.2180709562380032 Accuracy:  0.93835\n",
      "Iteration:  26150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26150 Loss:  0.21803809223213858 Accuracy:  0.9383333333333334\n",
      "Iteration:  26200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26200 Loss:  0.21800522876190415 Accuracy:  0.9383833333333333\n",
      "Iteration:  26250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26250 Loss:  0.21797231329149666 Accuracy:  0.9384166666666667\n",
      "Iteration:  26300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26300 Loss:  0.2179394465163429 Accuracy:  0.9384333333333333\n",
      "Iteration:  26350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26350 Loss:  0.2179066309024185 Accuracy:  0.93845\n",
      "Iteration:  26400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26400 Loss:  0.21787383488253242 Accuracy:  0.9384333333333333\n",
      "Iteration:  26450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26450 Loss:  0.21784106481438795 Accuracy:  0.9384333333333333\n",
      "Iteration:  26500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26500 Loss:  0.21780836552556346 Accuracy:  0.9384166666666667\n",
      "Iteration:  26550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26550 Loss:  0.2177757300713922 Accuracy:  0.9384166666666667\n",
      "Iteration:  26600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26600 Loss:  0.21774315340720207 Accuracy:  0.93845\n",
      "Iteration:  26650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26650 Loss:  0.21771064106973437 Accuracy:  0.9384666666666667\n",
      "Iteration:  26700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26700 Loss:  0.21767814590081339 Accuracy:  0.9384666666666667\n",
      "Iteration:  26750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26750 Loss:  0.21764566065316196 Accuracy:  0.9384666666666667\n",
      "Iteration:  26800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26800 Loss:  0.21761323871996535 Accuracy:  0.9384666666666667\n",
      "Iteration:  26850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26850 Loss:  0.2175809090400637 Accuracy:  0.9384333333333333\n",
      "Iteration:  26900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26900 Loss:  0.21754867960289018 Accuracy:  0.9384166666666667\n",
      "Iteration:  26950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  26950 Loss:  0.2175165152059712 Accuracy:  0.93845\n",
      "Iteration:  27000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27000 Loss:  0.21748442599528223 Accuracy:  0.9384833333333333\n",
      "Iteration:  27050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27050 Loss:  0.21745240386793124 Accuracy:  0.9384833333333333\n",
      "Iteration:  27100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27100 Loss:  0.2174202975403649 Accuracy:  0.9385\n",
      "Iteration:  27150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27150 Loss:  0.2173881479156468 Accuracy:  0.9385\n",
      "Iteration:  27200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27200 Loss:  0.21735609221338584 Accuracy:  0.9384833333333333\n",
      "Iteration:  27250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27250 Loss:  0.21732410961672544 Accuracy:  0.93845\n",
      "Iteration:  27300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27300 Loss:  0.2172921946683873 Accuracy:  0.93845\n",
      "Iteration:  27350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27350 Loss:  0.21726035188781612 Accuracy:  0.9384666666666667\n",
      "Iteration:  27400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27400 Loss:  0.21722856921787137 Accuracy:  0.9384833333333333\n",
      "Iteration:  27450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27450 Loss:  0.21719682476482768 Accuracy:  0.93845\n",
      "Iteration:  27500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27500 Loss:  0.21716510624502394 Accuracy:  0.9384666666666667\n",
      "Iteration:  27550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27550 Loss:  0.21713344931265818 Accuracy:  0.9384666666666667\n",
      "Iteration:  27600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27600 Loss:  0.21710183237114114 Accuracy:  0.9385\n",
      "Iteration:  27650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27650 Loss:  0.2170702702171909 Accuracy:  0.9385166666666667\n",
      "Iteration:  27700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27700 Loss:  0.21703871695278384 Accuracy:  0.9385\n",
      "Iteration:  27750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27750 Loss:  0.2170072516935063 Accuracy:  0.9385\n",
      "Iteration:  27800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27800 Loss:  0.216975848847059 Accuracy:  0.9385166666666667\n",
      "Iteration:  27850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27850 Loss:  0.21694448560539312 Accuracy:  0.9385166666666667\n",
      "Iteration:  27900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27900 Loss:  0.21691317371184043 Accuracy:  0.9385\n",
      "Iteration:  27950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  27950 Loss:  0.21688196515465485 Accuracy:  0.93855\n",
      "Iteration:  28000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28000 Loss:  0.21685082650156734 Accuracy:  0.9385833333333333\n",
      "Iteration:  28050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28050 Loss:  0.2168196973916547 Accuracy:  0.9386166666666667\n",
      "Iteration:  28100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28100 Loss:  0.21678854432623365 Accuracy:  0.9386\n",
      "Iteration:  28150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28150 Loss:  0.2167573960889631 Accuracy:  0.9385666666666667\n",
      "Iteration:  28200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28200 Loss:  0.2167262520224311 Accuracy:  0.9385666666666667\n",
      "Iteration:  28250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28250 Loss:  0.21669511238079672 Accuracy:  0.9385666666666667\n",
      "Iteration:  28300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28300 Loss:  0.2166640603142837 Accuracy:  0.9385666666666667\n",
      "Iteration:  28350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28350 Loss:  0.2166330584510468 Accuracy:  0.9385833333333333\n",
      "Iteration:  28400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28400 Loss:  0.2166020696698342 Accuracy:  0.9386333333333333\n",
      "Iteration:  28450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28450 Loss:  0.21657110413159958 Accuracy:  0.9386166666666667\n",
      "Iteration:  28500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28500 Loss:  0.21654019263699426 Accuracy:  0.9386333333333333\n",
      "Iteration:  28550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28550 Loss:  0.21650939319791906 Accuracy:  0.9386666666666666\n",
      "Iteration:  28600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28600 Loss:  0.21647868819215835 Accuracy:  0.9386833333333333\n",
      "Iteration:  28650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28650 Loss:  0.2164480528826355 Accuracy:  0.9387\n",
      "Iteration:  28700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28700 Loss:  0.21641738197482804 Accuracy:  0.9387\n",
      "Iteration:  28750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28750 Loss:  0.21638674455020473 Accuracy:  0.9386666666666666\n",
      "Iteration:  28800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28800 Loss:  0.2163562004177275 Accuracy:  0.9386666666666666\n",
      "Iteration:  28850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28850 Loss:  0.21632566516428337 Accuracy:  0.93865\n",
      "Iteration:  28900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28900 Loss:  0.21629504970579227 Accuracy:  0.9386666666666666\n",
      "Iteration:  28950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  28950 Loss:  0.2162644329830597 Accuracy:  0.9386833333333333\n",
      "Iteration:  29000\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29000 Loss:  0.21623385501470138 Accuracy:  0.9386666666666666\n",
      "Iteration:  29050\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29050 Loss:  0.21620332974620746 Accuracy:  0.9386833333333333\n",
      "Iteration:  29100\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29100 Loss:  0.2161728655789446 Accuracy:  0.9387\n",
      "Iteration:  29150\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29150 Loss:  0.2161424520470892 Accuracy:  0.9386833333333333\n",
      "Iteration:  29200\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29200 Loss:  0.21611204489109098 Accuracy:  0.9387166666666666\n",
      "Iteration:  29250\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29250 Loss:  0.21608168461679844 Accuracy:  0.9387166666666666\n",
      "Iteration:  29300\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29300 Loss:  0.21605137044621206 Accuracy:  0.9387\n",
      "Iteration:  29350\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29350 Loss:  0.21602109050868729 Accuracy:  0.9387\n",
      "Iteration:  29400\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29400 Loss:  0.21599086140555232 Accuracy:  0.9387166666666666\n",
      "Iteration:  29450\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29450 Loss:  0.215960698277143 Accuracy:  0.9387166666666666\n",
      "Iteration:  29500\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29500 Loss:  0.21593060896749072 Accuracy:  0.9387166666666666\n",
      "Iteration:  29550\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29550 Loss:  0.21590051810641483 Accuracy:  0.9387166666666666\n",
      "Iteration:  29600\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29600 Loss:  0.21587042134833656 Accuracy:  0.93875\n",
      "Iteration:  29650\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29650 Loss:  0.21584028333253752 Accuracy:  0.9387666666666666\n",
      "Iteration:  29700\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29700 Loss:  0.2158101648745738 Accuracy:  0.93875\n",
      "Iteration:  29750\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29750 Loss:  0.21578011926177762 Accuracy:  0.93875\n",
      "Iteration:  29800\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29800 Loss:  0.21575014728985603 Accuracy:  0.9387333333333333\n",
      "Iteration:  29850\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29850 Loss:  0.2157202024018491 Accuracy:  0.9387333333333333\n",
      "Iteration:  29900\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29900 Loss:  0.21569026567859106 Accuracy:  0.9387833333333333\n",
      "Iteration:  29950\n",
      "[5 0 4 ... 5 6 8] [5 0 4 ... 5 6 8]\n",
      "Iteration:  29950 Loss:  0.2156603730247355 Accuracy:  0.9388\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(train_X_flattened, train_y, iterations, alpha, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X, W1, b1, W2, b2):\n",
    "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    predictions = get_predictions(A2)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def test_prediction(index, W1, b1, W2, b2):\n",
    "    # Extracting the image and reshaping it to column vector\n",
    "    current_image = train_X[index].reshape(784, 1) / 255.0\n",
    "    prediction = make_predictions(current_image, W1, b1, W2, b2)\n",
    "    label = train_y[index]\n",
    "    \n",
    "    print(\"Prediction: \", prediction[0])  # [0] to get the scalar value\n",
    "    print(\"Label: \", label)\n",
    "\n",
    "    # Reshaping the image for visualization\n",
    "    current_image = current_image.reshape((28, 28))\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  5\n",
      "Label:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_prediction(0, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import util "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_image(image, path, name):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_name = os.path.join(path, name + \".png\")\n",
    "    cv2.imwrite(file_name, image)\n",
    "    print(f\"Saved processed image to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(ROI, save_image=False, image_path='', image_name='image'):\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(ROI, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply binary thresholding\n",
    "    _, thresholded_image = cv2.threshold(gray_image, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Resize the image to 28x28\n",
    "    resized_image = cv2.resize(thresholded_image, (28, 28))\n",
    "\n",
    "    # Invert the grayscale image\n",
    "    inverted_image = cv2.bitwise_not(resized_image)\n",
    "\n",
    "    # Flatten the resized (and potentially inverted) image\n",
    "    flattened_image = inverted_image.flatten().reshape((784, 1))\n",
    "\n",
    "    # Save the processed image\n",
    "    if save_image:\n",
    "        cv2.imwrite(f'{image_path}/{image_name}.png', inverted_image)\n",
    "\n",
    "    return flattened_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"\n",
    "    Preprocess the frame to match MNIST dataset format.\n",
    "\n",
    "    frame: Captured image from the camera.\n",
    "    \n",
    "    Returns:\n",
    "    Processed image ready for prediction.\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    if len(frame.shape) == 3 and frame.shape[2] == 3:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Resize to 28x28 (MNIST format)\n",
    "    frame_resized = cv2.resize(frame, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Invert colors if necessary (digit should be white on black background)\n",
    "    frame_inverted = cv2.bitwise_not(frame_resized)\n",
    "\n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    frame_normalized = frame_inverted / 255.0\n",
    "\n",
    "    # Flatten the image to a 784-element array\n",
    "    frame_flattened = frame_normalized.flatten().reshape(784, 1)\n",
    "\n",
    "    return frame_flattened\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_and_predict(W1, b1, W2, b2):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    ROI_size = 200\n",
    "    save_next_frame = False\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame, ROI = capture_frame(cap, ROI_size)\n",
    "        if not ret:\n",
    "            capture_and_predict(W1, b1, W2, b2)\n",
    "\n",
    "        # Check for space bar press to save the next frame\n",
    "        key = cv2.waitKey(10)\n",
    "        if key == 32:  # Space bar key code\n",
    "            save_next_frame = True\n",
    "\n",
    "        processed_ROI = preprocess_frame(ROI, save_image=save_next_frame, \n",
    "                                         image_path='images/processed_images/', \n",
    "                                         image_name='processed_img_{}'.format(frame_count))\n",
    "\n",
    "        # Reset save_next_frame after saving\n",
    "        if save_next_frame:\n",
    "            save_next_frame = False\n",
    "            frame_count += 1\n",
    "\n",
    "        predicted_digit = make_predictions(processed_ROI, W1, b1, W2, b2)[0]\n",
    "        display_prediction(frame, predicted_digit, ROI_size)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def capture_frame(cap, ROI_size):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        return False, frame, None\n",
    "\n",
    "    x_start, y_start = calculate_ROI_start(cap, ROI_size)\n",
    "    ROI = frame[y_start:y_start + ROI_size, x_start:x_start + ROI_size]\n",
    "    draw_ROI(frame, x_start, y_start, ROI_size)\n",
    "    return True, frame, ROI\n",
    "\n",
    "def calculate_ROI_start(cap, ROI_size):\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    x_start = (frame_width - ROI_size) // 2\n",
    "    y_start = (frame_height - ROI_size) // 2\n",
    "    return x_start, y_start\n",
    "\n",
    "def draw_ROI(frame, x_start, y_start, ROI_size):\n",
    "    cv2.rectangle(frame, (x_start, y_start), (x_start + ROI_size, y_start + ROI_size), (0, 255, 0), 3)\n",
    "\n",
    "def display_prediction(frame, predicted_digit, ROI_size):\n",
    "    cv2.putText(frame, f'Pred: {predicted_digit}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('Digit Recognition', frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to initialize your model parameters (W1, b1, W2, b2) before calling this function\n",
    "capture_and_predict(W1, b1, W2, b2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
